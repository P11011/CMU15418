视频地址：https://www.bilibili.com/video/BV1qT411g7n9?p=9&vd_source=bf04fffc5730343626bca0f42d47446b

第xx课是按视频顺序，并非PPT顺序

# 第1课：为什么需要并行计算？

---

#### 1. 并行计算的动机 (Motivation for Parallel Computing)
- **加速计算** (Speedup)：并行计算的主要目的是通过多个处理器协同工作来加速问题的求解。
  - **加速比** (Speedup)：定义为单个处理器的执行时间与多个处理器执行同一任务的时间之比。

  公式：

  $$
  \text{Speedup} = \frac{\text{单处理器执行时间}}{\text{多处理器执行时间}}
  $$

#### 2. 课堂演示与观察 (Class Demos and Observations)
- **演示 1：简单并行程序** (Demo 1: First Parallel Program)
  - **通信成本** (Communication Cost)：在并行计算中，通信会限制最大加速比，演示中每个“处理器”之间的通信显著影响了整体的加速效果。
  - **减小通信成本** (Minimizing Communication)：通过减少通信距离（例如让“处理器”靠近）或者使用更高效的通信方式可以显著提高加速效果。

- **演示 2：扩展到四个处理器** (Demo 2: Scaling to Four Processors)
  - **任务分配不均衡** (Imbalance in Work Assignment)：某些“处理器”完成任务后闲置，而其他处理器仍在工作，这导致了整体的加速效果受限。
  - **改进任务分配** (Improving Work Distribution)：更合理地分配任务可以减少空闲时间，从而提高加速效果。

- **演示 3：大规模并行执行** (Demo 3: Massively Parallel Execution)
  - **通信主导计算** (Communication Dominates Computation)：如果通信成本相对于计算量较大，那么通信开销会主导整个并行计算，严重限制性能提升。

#### 3. 课程主题 (Course Themes)
- **主题 1：设计并编写可扩展的并行程序** (Designing and Writing Scalable Parallel Programs)
  - **并行思维** (Parallel Thinking)：
    1. 将任务分解为可以安全并行执行的部分 (Decompose work into parallelizable parts)。
    2. 将任务分配给多个处理器 (Assign work to processors)。
    3. 管理处理器之间的通信与同步，避免限制加速比 (Manage communication and synchronization)。
  - **编程抽象和机制** (Abstractions and Mechanisms)：学习如何使用流行的并行编程语言来实现上述任务。

- **主题 2：并行计算机硬件实现** (Parallel Computer Hardware Implementation)
  - **硬件实现的机制** (Mechanisms for Efficient Implementation)：学习硬件如何高效实现并行抽象，并理解硬件设计中的**性能权衡** (Performance Trade-offs)，如**性能** (Performance)、**便利性** (Convenience) 和 **成本** (Cost) 之间的平衡。
  - **硬件的重要性** (Why Know About Hardware)：了解硬件特性对于编写高效并行程序至关重要，例如通信速度的限制。

- **主题 3：效率思维** (Thinking About Efficiency)
  - **快 ≠ 高效** (Fast ≠ Efficient)：一个程序在并行计算机上运行得更快，不代表它高效利用了硬件资源。例如，使用 10 个处理器却只获得 2 倍加速，说明硬件没有被有效利用。
  - **程序员与硬件设计者的视角** (Programmer vs. Hardware Designer Perspectives)：程序员要充分利用硬件提供的能力，而硬件设计者则要选择合适的组件来实现最优的性能/成本比。

#### 4. 历史背景与并行化原因 (Historical Context and Reasons for Parallelism)
- **过去的 CPU 性能提升来源** (Past CPU Performance Improvements)：
  1. **指令级并行 (ILP, Instruction-Level Parallelism)**：通过超标量执行 (Superscalar Execution) 来同时执行多个独立指令，从而加速程序。
  2. **提高时钟频率 (Frequency Scaling)**：通过增加 CPU 的时钟频率来提升性能。
  
- **功耗墙的限制** (The Power Wall)：由于动态功耗与电容负载、频率和电压的平方成正比，功耗成为现代处理器设计的关键瓶颈，导致频率难以继续提高。  
  - **动态功耗公式** (Dynamic Power Formula)：

  $$
  \text{Power} \propto \text{Capacitive Load} \times \text{Voltage}^{2} \times \text{Frequency}
  $$

  - 因此，**提高核心数量**成为提升性能的主要手段。

- **并行化的必要性** (Necessity for Parallelization)：单线程性能增长几乎停滞，现代计算机通过增加核心数和专用硬件单元（如 GPU）来提升整体性能。编写并行代码成为获得高性能的唯一途径。

#### 5. 现代并行计算硬件示例 (Modern Parallel Hardware Examples)
- **Intel Skylake**：四核 CPU 和多核 GPU 集成在同一芯片上（“第六代 Core i7”）。
- **Intel Xeon Phi 7290**：包含 72 个核心的协处理器 (Coprocessor)，用于大规模并行计算。
- **NVIDIA Maxwell GTX 1080 GPU**：拥有多个大型处理块，可用于大规模数据并行处理。

#### 6. 移动和超级计算领域的并行处理 (Mobile and Supercomputing Parallel Processing)
- **移动设备**：
  - **NVIDIA Tegra X1**：集成了 4 个 ARM A57 核心和 4 个 ARM A53 核心，以及 NVIDIA GPU 和图像处理器。
  - **Apple A9**：用于 iPhone 6s，包含双核 CPU 和 GPU。
  - **Raspberry Pi 3**：采用四核 ARM A53 处理器。

- **超级计算**：
  - **Oak Ridge National Laboratory 的 Titan 超级计算机**：由 18,688 个 16 核 AMD CPU 和 18,688 个 NVIDIA K20X GPU 组成的集群。

#### 7. 总结 (Summary)
- **单线程性能改进非常缓慢** (Single-thread Performance is Improving Slowly)：要显著提高程序运行速度，必须利用多个处理单元，这就要求编写**并行代码**。
- **编写并行程序的挑战** (Challenges in Writing Parallel Programs)：包括问题划分 (Problem Partitioning)、通信 (Communication) 和同步 (Synchronization)，还需要了解机器的硬件特性。
- **现代计算机的强大潜力** (Tremendous Potential of Modern Computers)：现代计算机拥有比人们想象中更强大的计算能力，只要你学会如何利用它们。





















































# 第2课：现代多核处理器的架构基础

---

## 1. 课程概述
- 本次课程主要讨论现代计算机的四个关键概念，其中两个与并行执行相关，两个与内存访问的挑战相关。
- 了解这些架构基础知识将帮助我们：
  - 理解和优化并行程序的性能。
  - 获得对于何种工作负载适合快速并行机器的直觉。

---

## 2. 单指令流性能提升缓慢的原因
- **单指令流（Single-instruction Stream）**的性能在近年来提高得非常缓慢，原因在于以下几点：
  1. **功耗墙（Power Wall）**：提升频率带来了功耗的迅速增加，限制了频率的继续提升。
  2. **指令级并行（ILP, Instruction Level Parallelism）**的潜力有限，现代处理器在探索指令并行性上已经接近瓶颈。

### 超标量处理器的指令顺序执行
- **超标量处理器（Superscalar Processor）**：可以同时解码和执行多条指令，但必须尊重指令的依赖关系。
- 例如：
  ```c
  a = 2;
  b = 4;
  tmp2 = a + b;        // 6
  tmp3 = tmp2 + a;     // 8
  tmp4 = b + b;        // 8
  tmp5 = b * b;        // 16
  tmp6 = tmp2 + tmp4;  // 14
  tmp7 = tmp5 + tmp6;  // 30

这些指令之间存在依赖，必须按照特定顺序执行。

## 3. 并行执行（Parallel Execution）

### 3.1 多核处理器的兴起

- 现代处理器通过增加核心数量，而非提升每个核心的复杂性，来提高性能。
- 每个核心运行速度可能比之前的“高级”核心慢一些，但多个核心并行执行可以带来显著的加速。

### 3.2 多线程并行（Multi-core Parallelism）

- 利用多个核心，每个核心可以运行一个独立的指令流。
- 程序需要明确地**创建线程**来利用多核，例如使用 **pthreads** API。

### 3.3 SIMD（单指令多数据）执行

- **SIMD（Single Instruction Multiple Data）**：一个指令同时操作多个数据。多个ALU共享同一条指令，从而提高计算能力。
- **向量化（Vectorization）**：可以通过编译器在编译时完成（显式SIMD），或由硬件在运行时完成。

### 3.4 超标量（Superscalar）与指令级并行（ILP）

- **超标量处理器**能够在同一时钟周期内处理来自同一指令流的多条指令，自动发现并行性并动态地执行它们。

------

## 4. 访问内存（Accessing Memory）

### 4.1 内存延迟与带宽

- **内存延迟（Memory Latency）**：处理器从内存系统获取请求数据所需要的时间，例如100个周期。
- **内存带宽（Memory Bandwidth）**：内存系统向处理器提供数据的速率，例如20 GB/s。

### 4.2 缓存的重要性

- 缓存（Cache）用于减少内存访问的延迟：
  - **L1缓存**：最快，但容量最小。
  - **L2缓存、L3缓存**：速度逐渐变慢，但容量更大。
- 通过缓存，可以有效减少内存访问的延迟，使处理器运行更加高效。

### 4.3 多线程隐藏延迟

- 多线程可以通过在一个线程等待内存访问时切换到另一个线程来**隐藏延迟**，这与缓存减少延迟的机制有所不同。

------

## 5. SIMD 向量化示例

- 使用

  AVX指令集

  的向量化示例：

  ```
  c复制代码#include <immintrin.h>
  
  void sinx(int N, int terms, float* x, float* result) 
  {
      float three_fact = 6;  // 3!
      for (int i = 0; i < N; i += 8) 
      {
          __m256 origx = _mm256_load_ps(&x[i]);
          __m256 value = origx;
          __m256 numer = _mm256_mul_ps(origx, _mm256_mul_ps(origx, origx));
          __m256 denom = _mm256_broadcast_ss(&three_fact);
          int sign = -1;
  
          for (int j = 1; j <= terms; j++) 
          {
              __m256 tmp = _mm256_div_ps(_mm256_mul_ps(_mm256_set1_ps(sign), numer), denom);
              value = _mm256_add_ps(value, tmp);
              numer = _mm256_mul_ps(numer, _mm256_mul_ps(origx, origx));
              denom = _mm256_mul_ps(denom, _mm256_broadcast_ss((2 * j + 2) * (2 * j + 3)));
              sign *= -1;
          }
          _mm256_store_ps(&result[i], value);
      }
  }
  ```

  - 该程序利用AVX指令来实现对向量的并行操作，每次处理8个32位浮点数，从而显著提高性能。

------

## 6. 现代处理器架构中的并行性

- 现代处理器通过结合**多核、多线程、SIMD**来实现并行性，从而最大化利用计算资源。
- **多核**提供线程级并行（Thread-level Parallelism）。
- **SIMD**在单个核心内部实现数据级并行（Data-level Parallelism）。
- **超标量**处理器则在指令流内部利用指令级并行（ILP）。

### 6.1 处理器示例

- **Intel i7-7700K**：4核，每核8个SIMD ALU，能够提供高达268 GFLOPs的计算性能。
- **NVIDIA GTX 1080**：20个计算单元，每个单元包含128个SIMD ALU，理论计算性能达8.1 TFLOPs。

------

## 7. 课程总结

- **并行执行的形式**：
  1. **多核（Multi-core）**：通过多个核心实现线程级并行。
  2. **SIMD**：利用单指令多数据方式在同一时间对多个数据进行操作。
  3. **超标量（Superscalar）**：在指令流内部发现并行性。
- **内存访问的挑战**：
  - **内存延迟**和**带宽**都是制约并行程序性能的重要因素，合理利用缓存和多线程技术是提高性能的关键。
- **计算性能的限制**：
  - 现代并行应用通常受限于内存带宽而非算力，因此需要提高程序的**算术强度（Arithmetic Intensity）**，即每次数据访问所做的计算量。

































































# 第3课：并行编程模型（Parallel Programming Models）

------

## 1. 抽象与实现的区别

- **抽象（Abstraction）**：抽象是程序员编写并行程序时使用的逻辑模型，用于简化复杂性并隐藏硬件的细节。
- **实现（Implementation）**：实现是如何在硬件上实现这些抽象的具体方式。

理解抽象和实现之间的区别对于编写并行程序非常重要。混淆这两者是本课程中常见的错误原因。

------

## 2. ISPC 介绍

- **ISPC** 是 **Intel SPMD Program Compiler** 的缩写，用于编写基于 **单程序多数据（SPMD, Single Program Multiple Data）** 模型的并行代码。
- ISPC 的编程模型基于 SPMD 抽象，调用 ISPC 函数会产生一个**gang**，即一组并行执行的“程序实例”（program instances），这些实例并发地运行相同的 ISPC 代码。

### 代码示例：使用泰勒展开计算 `sin(x)`

- 使用 ISPC 编写对数组中每个元素计算 `sin(x)` 的并行版本，使用 `foreach` 结构遍历每个元素。
- 关键字 **`uniform`** 用于表示所有程序实例共享的值，以便优化并行性能。

```
c复制代码export void sinx(
    uniform int N,
    uniform int terms,
    uniform float* x,
    uniform float* result)
{
    foreach (i = 0 ... N) {
        float value = x[i];
        float numer = x[i] * x[i] * x[i];
        uniform int denom = 6;  // 3!
        uniform int sign = -1;

        for (uniform int j = 1; j <= terms; j++) {
            value += sign * numer / denom;
            numer *= x[i] * x[i];
            denom *= (2 * j + 2) * (2 * j + 3);
            sign *= -1;
        }
        result[i] = value;
    }
}
```

- **`foreach`** 是 ISPC 中的一个重要语言结构，声明并行的循环迭代。
- `programCount` 用于控制有多少个“程序实例”在 `gang` 中同时执行。

------

## 3. 三种并行编程模型（Parallel Programming Models）

### 3.1 共享地址空间模型（Shared Address Space Model）

- **共享变量**：所有线程可以读取和写入共享变量。
- **优点**：自然地扩展了单处理器的编程方式，易于理解。
- **缺点**：需要管理共享变量的访问，使用锁等同步机制，以避免数据竞争和性能下降。

### 3.2 消息传递模型（Message Passing Model）

- 每个线程在各自的私有地址空间内运行，线程之间通过**发送和接收消息**进行数据交换。
- **实现方式**：例如使用 **MPI（Message Passing Interface）** 库来实现消息传递。
- **优点**：结构化的通信便于理解线程之间的数据交换过程。
- **缺点**：编写初始程序较为复杂，但有利于编写可扩展的并行程序。

### 3.3 数据并行模型（Data Parallel Model）

- **数据并行性**意味着对数据集的每个元素执行相同的操作，类似于 **“map”** 操作，将函数映射到集合中的每个元素。
- **foreach** 结构在 ISPC 中用于数据并行模型，声明对数据集中的元素进行独立处理。
- **优点**：结构化强，便于并行化和优化。
- **缺点**：通信受到限制，只能并行执行彼此独立的迭代。

------

## 4. NUMA（非一致性内存访问，Non-Uniform Memory Access）

- 在 **NUMA** 架构中，所有处理器都可以访问整个内存空间，但访问不同区域内存的**代价（延迟或带宽）**不同。
- **优点**：访问本地内存的延迟较低，并且可以提供高带宽。
- **缺点**：需要程序员努力找到并利用数据的局部性，以获得最佳性能。

------

## 5. ISPC 的任务与 SIMD 实现

- ISPC 的 **gang** 抽象通过单指令多数据（**SIMD, Single Instruction Multiple Data**）指令在单个核心上实现。
- ISPC 还支持**任务（Task）**，用于在多个 CPU 核心上实现多核并行。
- 编译器生成的二进制文件包含 SIMD 指令，C++ 代码可以像调用其他函数一样链接这些对象文件。

------

## 6. ISPC 中的 `reduce_add`

- ISPC 提供了 **`reduce_add`** 函数，用于对多个程序实例的部分结果进行累加操作。
- 在并行执行的情况下，每个程序实例可以计算部分和，然后使用 **`reduce_add`** 来汇总这些部分和，从而得到整个数组的和。

示例代码：

```
c复制代码export uniform float sumall(uniform int N, uniform float* x) {
    uniform float sum;
    float partial = 0.0f;
    foreach (i = 0 ... N) {
        partial += x[i];
    }

    // 使用 ISPC 数学库中的 reduce_add
    sum = reduce_add(partial);
    return sum;
}
```

- 每个程序实例计算自己的**私有部分和**（partial sum），没有通信需求。
- 部分和通过 **`reduce_add()`** 进行汇总，得到相同的最终和。

------

## 7. 总结

- **编程模型（Programming Models）**提供了一种组织并行程序的方式，允许多种有效的实现。
- **共享地址空间模型**：通信是隐式的，编程方式自然，但容易导致错误。
- **消息传递模型**：所有通信都以消息的形式完成，结构化明显，有助于编写正确且可扩展的程序。
- **数据并行模型**：将函数映射到大数据集上，限制了迭代间的通信，保证了独立处理的并行性。
- **现代实践**：在集群的每个节点内部使用共享地址空间编程，节点之间使用消息传递进行通信。













































# 第4课：并行编程基础

- #### 1. **并行编程的思维过程**

  - 编写一个并行程序涉及以下几个步骤：

    1. 识别可以并行执行的工作

       （Identify parallelizable work）：

       - 找到哪些部分的任务可以同时执行。

    2. 划分工作和数据

       （Decomposition of work and data）：

       - 将整个问题分解为多个子任务，并将相关的数据分配给这些任务。

    3. 管理数据访问、通信和同步

       （Manage data access, communication, and synchronization）：

       - 确保并行任务间能够有效沟通并避免冲突。

  #### 2. **关键概念：分解、分配、编排和映射**

  - 分解（Decomposition）：
    - 将一个问题分解为多个可以并行执行的子任务，目标是创建足够多的任务来保持所有计算单元忙碌。
    - 关键是识别任务之间的依赖关系，或者缺少依赖的部分，使得这些部分可以安全地并行执行。
  - 分配（Assignment）：
    - 将这些子任务分配给不同的执行单元（例如线程），目标是**平衡负载**（balance load），减少通信开销。
    - 分配可以是**静态**的（编译时决定，static）或者**动态**的（运行时分配，dynamic）。
  - 编排（Orchestration）：
    - 编排指的是如何协调各个并行任务，包括**结构化通信**（structured communication）、**添加同步**（adding synchronization）来保持依赖关系、组织内存中的数据结构、以及调度任务的执行。
    - 目标是减少通信和同步的成本，并保持数据访问的局部性（data locality）。
  - 映射（Mapping）：
    - 映射指的是如何将并行线程（工作者）与硬件执行单元相匹配。例如，如何将线程映射到 CPU 核心上，或者将 CUDA 线程块映射到 GPU 核心上。

  #### 3. **ISPC 编程模型示例**

  - 静态任务分配（Static Task Allocation）：
    - 在 ISPC 中，程序实例（多个并行执行的代码）处理不同的数据部分。静态分配中，程序实例通过循环中的索引分配不同的任务。
    - 例如，在 `foreach` 循环中，每个 ISPC 实例处理数据中的某一部分，这种方式可以更好地利用硬件的 SIMD 能力。
  - 动态任务分配（Dynamic Task Allocation）：
    - 使用 ISPC 的 `launch` 关键字可以创建任务，这些任务会被系统动态地分配给可用的线程。这样做的好处是可以根据线程的空闲状态来分配任务，从而实现更好的负载均衡。

  #### 4. **Amdahl 定律（Amdahl's Law）**

  - Amdahl 定律

    描述了并行计算中性能提升的限制：

    - 如果程序中有部分工作是**串行的**（serial），那么即使增加并行处理器的数量，整体性能提升仍然会被这些串行部分所限制。
    - 设 `S` 为程序中必须串行执行的部分比例，则最大加速比为 `1/S`，即串行部分越大，整体加速潜力越小。

  #### 5. **并行计算示例**

  - 图像处理（Image Processing）：
    - 以对图像的每个像素进行亮度调整为例，这是一个天然可以并行化的任务，因为每个像素的操作是独立的。
    - 然而，后续步骤（如计算整个图像的平均亮度）可能需要先汇总各个线程的计算结果，因此包含**串行部分**，这限制了整体加速的效果。

  #### 6. **分配策略示例**

  - 在使用 ISPC 时，可以通过不同的方式将工作分配给线程：
    - 交错分配（Interleaved Assignment）：
      - 将数组中的元素交错地分配给各个线程。
    - 块状分配（Blocked Assignment）：
      - 将连续的数据块分配给每个线程，适用于需要保持数据连续性和缓存利用率的情况。
  - 动态任务管理（Dynamic Task Management）：
    - 在 ISPC 中，动态任务分配可以使得空闲的线程从任务池中获取新的任务，避免某些线程空闲而其他线程过载的情况。

  #### 7. **共享地址空间与同步（Shared Address Space and Synchronization）**

  - 共享地址空间（Shared Address Space）：
    - 多个线程通过共享内存进行通信和数据交换。为了避免数据冲突，必须使用**锁（lock）**或**屏障（barrier）**等同步机制。
  - 同步机制（Synchronization Mechanisms）：
    - **锁（Lock）**：用于保护临界区（critical section），确保同一时间只有一个线程能够访问共享资源。
    - **屏障（Barrier）**：所有线程在到达屏障时必须等待，直到所有线程都到达该点后，才能继续执行。这在分阶段的并行计算中非常重要。

  #### 8. **二维网格求解器的并行化（Parallelization of 2D Grid Solver）**

  - **问题描述**：解决偏微分方程（partial differential equations, PDE）的迭代方法，使用二维网格进行数值求解。
  - 依赖关系的处理：
    - 在求解过程中，每个网格点依赖于其邻居的值，这样的依赖关系决定了并行化的难度。
    - 为了实现并行，可以将网格分为不同颜色的点（例如红色和黑色），先更新所有红色点，再更新所有黑色点，这样可以减少同步的频率。

  #### 9. **提高并行性能的技巧**

  - 消除依赖性（Eliminating Dependencies）：

    - 通过对数据或任务的重新组织，可以减少任务之间的依赖性。例如，通过使用不同版本的共享变量来消除迭代之间的依赖，从而允许更多的并行化。

  - 减少同步（Reducing Synchronization）：

    - 频繁的同步会导致性能下降。通过将部分和累加到局部变量中，然后再进行全局累加，可以减少锁的使用次数，提升性能。

  #### 10. **总结与反思（Summary and Reflection）**

  - 并行编程的核心挑战在于**识别任务的并行性（Identify Parallelism）**、**管理任务之间的依赖和同步（Manage Dependencies and Synchronization）**，以及**有效地将任务分配到硬件上（Efficient Mapping to Hardware）**。
- **Amdahl 定律**提醒我们，串行部分的比例越大，整体并行化的收益就越小，因此在设计并行程序时，尽量减少串行部分是非常重要的。

















































# 第5课：性能优化第一部分 - 工作分配与调度

---

## 1. 课程概述
本节课的主题是如何优化并行程序的性能，重点讨论了工作分配（Work Distribution）与调度（Scheduling）的方法。课程包含以下主要内容：
- 并行程序性能优化的基本策略。
- 工作分配的不同方式：静态分配、动态分配、半静态分配。
- 通过合理的调度和分配策略达到平衡负载和降低同步开销的目标。

---

## 2. 并行程序性能优化的目标
在优化并行程序性能时，我们的目标是：
- **负载均衡**（Balance workload）：确保所有处理器在程序执行期间都保持工作状态。
- **减少通信**（Reduce communication）：避免由于通信而导致的停顿。
- **减少额外开销**（Reduce extra work）：避免为增加并行度、分配管理等而带来的额外计算负担。

这三个目标往往是相互冲突的，因此需要在不同策略之间进行权衡。

### 提示
> 总是先实现最简单的解决方案，然后测量性能，判断是否需要进行优化。这是一种迭代的优化过程。

---

## 3. 静态分配（Static Assignment）
### 3.1 什么是静态分配？
静态分配是指工作分配给线程是预先确定的，尽管具体的分配算法可能取决于运行时的参数（例如输入数据的大小、线程数等），但分配在执行前就已经固定。

### 3.2 适用场景
- 当**任务的执行时间和工作量是可预测的**时，可以使用静态分配。
- 例如，在矩阵求解的场景中，可以将网格单元均匀地分配给每个线程。

### 3.3 优缺点
- **优点**：实现简单，几乎没有运行时的额外开销。
- **缺点**：在任务量不均的情况下，可能导致负载不均衡。

---

## 4. 动态分配（Dynamic Assignment）
### 4.1 动态分配简介
动态分配是在程序运行期间决定工作分配，以确保负载均衡，适用于任务执行时间或任务总量**不可预测**的场景。

#### 示例代码：使用共享计数器的并行任务分配
```c
LOCK counter_lock;
int counter = 0;  // 共享变量

while (1) {
    int i;
    lock(counter_lock);
    i = counter++;
    unlock(counter_lock);
    if (i >= N)
        break;
    is_prime[i] = test_primality(x[i]);
}
```

- **关键部分**：使用锁来保护计数器，防止多个线程同时访问并修改。
- **缺点**：当任务粒度太小时，会导致高频的同步操作，增加了同步开销。

### 4.2 增加任务粒度

为了减少同步开销，可以通过增加任务的粒度来降低进入关键区的次数：

```
c复制代码const int GRANULARITY = 10;

while (1) {
    int i;
    lock(counter_lock);
    i = counter;
    counter += GRANULARITY;
    unlock(counter_lock);
    if (i >= N)
        break;
    int end = min(i + GRANULARITY, N);
    for (int j = i; j < end; j++)
        is_prime[j] = test_primality(x[j]);
}
```

通过增加任务粒度，每次分配多个任务，可以减少同步次数，从而降低同步带来的开销。

------

## 5. 动态分配中的智能调度

### 5.1 任务调度问题

- 如果系统按照任务的顺序分配给工作线程，可能会导致某个长任务最后被分配，进而产生负载不均衡的问题。
- 解决方案之一是将任务拆分为更小的部分，或者首先调度较长的任务，以减少整体计算结束时的“拖尾”效应。

### 5.2 工作窃取（Work Stealing）

为了降低单一共享工作队列导致的同步开销，可以为每个线程分配一个独立的工作队列。当本地队列为空时，线程会尝试从其他线程的队列中窃取任务。

- **每个线程维护自己的工作队列**：本地线程从队列尾部推送/弹出任务，其他线程从队列头部窃取任务。
- 这种方式减少了竞争，提高了任务调度的效率。

------

## 6. Fork-Join 并行模式

### 6.1 Fork-Join 概述

Fork-Join 模式是一种自然的方式来表达**分治算法**中的独立工作。使用这种模式可以有效地利用多核 CPU 的并行计算能力。

#### Cilk Plus 中的基本 Fork-Join 语法

- **`cilk_spawn`**：用于创建新任务，例如 `cilk_spawn foo();`，表示函数 `foo()` 可以与当前函数异步执行。
- **`cilk_sync`**：等待所有由当前函数派生出的任务完成。

```
c复制代码cilk_spawn foo();
bar();
cilk_sync;  // 等待 foo() 和 bar() 都完成
```

### 6.2 并行快速排序的例子

```
c复制代码void quick_sort(int* begin, int* end) {
    if (begin >= end - PARALLEL_CUTOFF)
        std::sort(begin, end);  // 小规模问题直接排序
    else {
        int* middle = partition(begin, end);
        cilk_spawn quick_sort(begin, middle);
        quick_sort(middle + 1, end);
    }
}
```

- 当问题规模足够小时，直接使用串行排序以减少并行开销。
- 当问题较大时，使用 `cilk_spawn` 创建新的并行任务。

------

## 7. 工作窃取调度器的实现

### 7.1 选择执行子任务或继续执行主任务

当遇到 `cilk_spawn` 时，有两种策略：

- **执行子任务，延续主任务**：这种策略会让子任务尽早执行，并将主任务放入工作队列中供其他线程窃取。
- **继续执行主任务**，将子任务放入工作队列。这种策略称为“子任务窃取”（child stealing），可以减少工作队列中的任务数。

### 7.2 工作队列的实现

- 每个工作线程维护一个双端队列（dequeue），本地线程从队尾操作，而其他线程从队头窃取任务。
- **窃取策略**：空闲的线程会随机选择一个工作线程进行窃取，从队列头部窃取最顶层的任务，确保最大限度减少窃取的次数并提高数据局部性。

------

## 8. 课程总结

- **负载均衡**：我们希望所有处理器在执行期间始终保持工作状态，以充分利用资源，但同时需要降低实现这种平衡所需的成本。
- **静态 vs. 动态分配**：实际应用中，静态与动态分配的选择并非二元对立，而是一个连续的选择过程，应根据任务的特性做出最合适的决策。
- **Fork-Join 并行模式**：适用于分治算法的并行化，Cilk Plus 提供了一种简洁的抽象来实现这一模式。
- **工作窃取调度器**：通过工作窃取机制实现高效的负载均衡，保证线程尽可能保持繁忙状态，减少资源浪费。







































# 第6课：GPU 架构与 CUDA 编程

---

## 1. 课程概述

本节课的主要内容包括：

- **GPU 的历史**：从最初设计用于加速 3D 游戏（例如《Quake》），到演变为广泛应用于深度学习、计算机视觉和科学计算的高性能计算引擎。
- **CUDA 编程语言的使用**：学习如何使用 CUDA 进行 GPU 编程。
- **GPU 架构的详细分析**：深入了解现代 GPU 架构的关键特性。

---

## 2. GPU 架构概述

- **GPU 内存系统**：
  - 采用 **DDR5 DRAM**，通常具有数 GB 的容量。
  - 高端 GPU 的内存带宽可达 150-300 GB/s。

- **多核芯片（Multi-core Chip）**：
  - 每个核心内部执行 **SIMD（单指令多数据，Single Instruction Multiple Data）**，即每个核心中有多个执行单元可以同时执行相同的指令。
  - 每个核心可以同时执行多个线程，具有多线程执行能力。

---

## 3. GPU 的演化历史

### 3.1 早期 GPU 的目标

- **3D 渲染**：最早的 GPU 设计目标是为了实现 3D 场景的渲染，包括处理三角形网格、材质、光源等，最终输出图像(专门计算三角形---三角形是平面上最简单的多边形，任何复杂的形状都可以通过分割为多个三角形来表示，三角形网格可用于描述几乎所有 3D 模型和场景，且三角形的几何特性（如总是平面、三个顶点确定一个面）使得光栅化和变换操作更高效。GPU 能快速地计算顶点和进行透视投影)。

### 3.2 现代 GPU 的用途

- 目前 GPU 仍然用于实时图形渲染，例如 **《Ryse: Son of Rome》** 和 **《Unreal Engine Kite Demo》** 的实时渲染（每秒 30 帧）。
- GPU 也用于其他计算密集型任务，如深度学习、科学计算等。

---

## 4. 实时图形管线（Real-time Graphics Pipeline）

GPU 的图形处理任务可以抽象为对一系列图元（如三角形、像素等）的连续操作：

1. **顶点生成（Vertex Generation）**：输入 3D 顶点数据，生成顶点流。
2. **顶点处理（Vertex Processing）**：根据摄像机位置和方向，将 3D 顶点映射到屏幕上的 2D 坐标。
3. **图元生成（Primitive Generation）**：将顶点组合成基本图元（如三角形）。
4. **片段生成（Fragment Generation / Rasterization）**：为每个被图元覆盖的像素生成一个片段。
5. **片段处理（Fragment Processing）**：计算片段的颜色，基于材质和光照模型。
6. **像素操作（Pixel Operations）**：将最近的片段颜色写入输出图像的像素缓冲区。

---

## 5. CUDA 编程简介

### 5.1 CUDA 的历史

- **CUDA** 于 2007 年由 NVIDIA 推出，伴随着 **Tesla 架构**，作为一种用于 GPU 编程的 C 语言扩展。
- **CUDA 的低级抽象**：CUDA 的抽象与现代 GPU 的硬件特性紧密结合，保持低抽象距离，以便更高效地控制 GPU。

### 5.2 CUDA 编程模型

- **线程层次结构（Thread Hierarchy）**：CUDA 程序由多个线程组成，这些线程被组织为线程块（Thread Block）。
- **线程块（Thread Block）**：线程块内的线程可以共享内存并进行同步。线程块之间的执行顺序不保证，但线程块内部的线程必须协同工作。
- **线程 ID**：线程可以通过其在块内的位置（`threadIdx`）和块在整个网格中的位置（`blockIdx`）来确定其全局 ID。

### 5.3 CUDA 示例代码

```c
#define THREADS_PER_BLK 128

__global__ void matrixAdd(float A[Ny][Nx], float B[Ny][Nx], float C[Ny][Nx]) {
    int i = blockIdx.x * blockDim.x + threadIdx.x; //分块编号*分块长+当前下标
    int j = blockIdx.y * blockDim.y + threadIdx.y; //分块编号*分块宽+当前下标

    C[j][i] = A[j][i] + B[j][i];
}
```

- 在这个示例中，矩阵加法通过并行执行来加速，每个线程负责计算结果矩阵的一个元素。

------

## 6. CUDA 内存模型

- **主机（Host）内存**与**设备（Device）内存**是分开的。程序需要通过 **`cudaMemcpy`** 将数据在两者之间移动。

- 设备内存

  有三种不同类型的地址空间：

  - **每线程私有内存（Per-thread Private Memory）**：仅对单个线程可见。
  - **每块共享内存（Per-block Shared Memory）**：块内所有线程可读写。
  - **全局内存（Global Memory）**：所有线程均可读写。

### 6.1 内存拷贝示例

```
c复制代码float* A = new float[N]; // 在主机内存中分配缓冲区
cudaMalloc(&deviceA, sizeof(float) * N); // 在设备内存中分配缓冲区

cudaMemcpy(deviceA, A, sizeof(float) * N, cudaMemcpyHostToDevice); // 将数据从主机复制到设备
```

------

## 7. CUDA 同步与调度

- **线程同步（Thread Synchronization）**：使用 **`__syncthreads()`** 在块内同步线程，确保所有线程在继续之前都已完成特定步骤。
- **原子操作（Atomic Operations）**：例如 **`atomicAdd()`**，用于防止多个线程同时修改共享变量时引发数据竞争。
- **调度策略**：GPU 使用动态调度策略将线程块分配给可用的核心。

### 7.1 CUDA 中的卷积示例

```
#define THREADS_PER_BLK 128

__global__ void convolve(int N, float* input, float* output) {
    __shared__ float support[THREADS_PER_BLK + 2]; // 每块分配共享内存
    int index = blockIdx.x * blockDim.x + threadIdx.x;

    support[threadIdx.x] = input[index];
    if (threadIdx.x < 2) {
        support[THREADS_PER_BLK + threadIdx.x] = input[index + THREADS_PER_BLK];
    }

    __syncthreads();

    float result = 0.0f;
    for (int i = 0; i < 3; i++)
        result += support[threadIdx.x + i];

    output[index] = result / 3.0f;
}
```

- 使用共享内存来减少全局内存访问次数，从而提高内存访问效率。

------

## 8. 重要概念总结

- **线程块（Thread Block）**之间是逻辑上的并发，系统可以以任意顺序调度这些线程块。
- **线程束（Warp）**：在 NVIDIA GPU 上，32 个 CUDA 线程共享一个指令流（SIM），被称为 **warp**（一个线程块可以有多个warp）。线程束的执行方式类似于 **SIMD**，共享相同的指令流，但各线程可能处理不同的数据（每个线程拥有独立的数据访问路径和独立的执行上下文。这种架构允许每个线程在同一个程序中处理不同的数据，从而实现更灵活的并行计算）。
- **线程块调度**：所有线程块中的线程会被映射到相同的核心，以便通过共享内存（块内线程共享）实现低延迟通信。

------

## 9. 课程总结

- CUDA 的执行模型：
  - 线程块的划分与数据并行模型类似，目的是使程序的执行与具体的硬件实现无关。
  - 线程块内部的线程是协作的并发执行，属于 SPMD（单程序多数据）共享地址空间编程。
- 内存模型：
  - CUDA 具有分布式地址空间，包括主机与设备内存之间的分离，以及设备内存内部的本地、共享和全局内存。
  - 在编写 CUDA 程序时，需要根据不同的内存类型合理安排数据的存储和读取，以优化性能。
- 实现细节：
  - 线程块中的线程被调度到相同的 GPU 核心，以实现高带宽和低延迟的共享内存通信。
  - 线程块内的线程被分组为 **warp**，以实现 GPU 硬件上的 SIMD 执行













































# 第7课：性能优化第二部分 - 局部性、通信与竞争



---

## 1. 课程概述
- 本节课讨论如何通过优化并行程序的通信、局部性和竞争来提高性能。
- 内容涵盖了如何**最小化通信成本**、**提高数据局部性**和**减少资源竞争**。

---

## 2. 消息传递模型（Message Passing Model）
- 在消息传递模型中，每个线程运行在其**私有地址空间（Private Address Space）**中。
- 线程之间通过**发送（send）和接收（receive）消息**进行通信和同步。
- 这种模型在没有共享地址空间的并行计算环境中尤为常见，例如计算集群。

### 2.1 栅格求解器示例
- 每个线程对一部分数据进行处理，这些数据存储在每个线程独立的地址空间中。
- 需要将边界数据复制到邻近线程，这些边界数据被称为**“幽灵单元”（Ghost Cells）**，它们是从远程线程地址空间复制来的。

---

## 3. 通信的两种类型
### 3.1 内在通信（Inherent Communication）
- **内在通信**是并行算法中必须发生的通信，属于算法本质的一部分。
- 例如，在栅格求解器中，更新某些网格单元时必须发送相邻线程的数据，这种通信是不可避免的。

### 3.2 虚假的通信（Artifactual Communication）
- **虚假的通信**是由于系统实现的限制而引入的额外通信。
- 例如，系统可能有最小的数据传输粒度。如果程序需要传输 4 字节的数据，但系统要求传输 64 字节的缓存行，就会导致比实际所需多 16 倍的通信量。

### 3.3 通信-计算比率（Communication-to-Computation Ratio）
- **通信-计算比率**衡量程序对带宽的需求：
  $$
  \text{Communication-to-Computation Ratio} = \frac{\text{通信量（字节数）}}{\text{计算量（指令数）}} 
  $$
- **算术强度（Arithmetic Intensity）**是其倒数：
  $$
  \text{Arithmetic Intensity} = \frac{\text{计算量}}{\text{通信量}}
  $$
  - **高算术强度**表示计算量远大于通信量，对于现代并行处理器的高效利用至关重要。

---

## 4. 提高局部性与减少通信
### 4.1 优化数据分配
- **块状分配（Blocked Assignment）**可以有效减少通信量，增加算术强度。
- 在二维网格上采用块状分配比一维分配更能捕获数据的**空间局部性（Spatial Locality）**，从而减少通信需求。

### 4.2 增加通信的粒度
- 合并小消息为大消息，减少通信次数，降低通信的额外开销。

---

## 5. 消息传递的同步与死锁问题
### 5.1 同步（Synchronous）与异步（Asynchronous）通信
- **同步通信（Synchronous Communication）**：`send()`调用在消息被对方接收并确认后返回，`recv()`等待接收完成。
- **异步通信（Asynchronous Communication）**：`send()`和`recv()`调用会立即返回，线程可以在等待消息发送或接收的同时进行其他工作。
- 异步通信可以实现**通信与计算的重叠（Overlap）**，从而隐藏通信延迟。

### 5.2 死锁问题与解决方案
- 使用同步通信时，如果两个线程都在等待对方的消息，会导致**死锁（Deadlock）**。
- 一种解决方案是采用交替发送和接收的策略。例如，偶数线程先发送后接收，而奇数线程则先接收后发送。

---

## 6. 管道化（Pipelining）提高吞吐量
### 6.1 示例：洗衣流程的管道化
- 将洗衣服的操作分为多个阶段（洗衣、烘干、折叠），这些操作可以同时进行，从而提高**吞吐量（Throughput）**。
- **延迟（Latency）**是完成单次操作所需的时间，而**吞吐量**是单位时间内完成的操作数量。

### 6.2 指令流水线（Instruction Pipeline）
- 在处理器中，指令被分解为多个小步骤（如取指、解码、执行、写回），这些步骤可以流水线化执行，从而提高每周期执行的指令数（IPC）。

---

## 7. 竞争与资源共享（Contention and Shared Resources）
### 7.1 竞争的定义
- **竞争（Contention）**发生在多个线程同时请求一个有限资源的情况下。例如，多个线程同时访问内存，或者同时更新一个共享变量。
- 解决竞争的方法包括**局部复制资源（Local Replication）**和**细粒度锁（Fine-Grained Locks）**。

### 7.2 竞争的解决方案
- **树型结构通信（Tree-Structured Communication）**：通过树型结构来汇总数据，减少并发请求的数量，降低竞争。
- **分布式工作队列（Distributed Work Queues）**：每个线程有自己的工作队列，当本地工作耗尽时，线程可以从其他线程的队列中窃取任务（**工作窃取，Work Stealing**），从而降低对全局共享队列的竞争。

### 7.3 CUDA 编程中的竞争
- 在 CUDA 中，所有线程块内的线程可以通过共享内存进行通信和同步。
- 如果所有线程都在等待同一个共享内存的加载操作，这会导致**竞争**，进而降低性能。解决方案是为每个核心分配多个线程块，使线程在等待时能有其他工作可做，从而隐藏内存延迟。

---

## 8. 课程总结
- **通信优化的三大方向**：
  1. **减少通信开销**：通过减少消息数量和合并小消息，降低发送和接收的开销。
  2. **减少通信延迟**：通过重组代码更好地利用数据局部性，或改进通信架构。
  3. **减少资源竞争**：复制资源，使用细粒度锁，以及错开对共享资源的访问时间。
  
- **提高并行程序性能的关键**：
  1. **利用数据局部性**，减少通信量并提高算术强度。
  2. **减少通信和同步的开销**，特别是在大规模并行环境下。
  3. **最大化通信与计算的重叠**，通过异步通信隐藏延迟。

































































# 第8课：并行编程案例研究

---

## 1. 课程概述
- 本节课通过多个实际并行应用的案例研究，展示了不同的并行编程技术及其优化方法。
- 涵盖的案例包括：
  - **海洋模拟（Ocean Simulation）**
  - **星系演化模拟（Galaxy Evolution - Barnes-Hut Algorithm）**
  - **并行扫描（Parallel Scan）**
  - **数据并行分段扫描（Data-parallel Segmented Scan）**
  - **光线追踪（Ray Tracing）**

---

## 2. 海洋模拟案例（Ocean Simulation）
### 2.1 问题描述
- 使用栅格求解器（Grid Solver）来模拟海洋电流。
- 将三维海洋体积离散为二维网格，每个时间步长 $\Delta t$ 进行更新（将三维海洋体积离散为二维网格时，通常基于一个假设：在一个较大的时间尺度下，海洋的垂直变化较小或与水平变化相比较慢。因此，可以将海洋体积划分为多层“切片”，每一层代表不同深度的二维网格，具体如下：
  1. **水平切片**：将海洋的水平面积离散为二维网格，通常使用经纬度来定义每个网格单元。
  2. **多层处理**：每层表示一个不同深度的海洋体积，所有层组合起来形成一个三维的海洋模型。
- ）。
- **高精度**模拟要求较小的时间步长和高分辨率网格。

### 2.2 并行实现
- **数据分区（Decomposition）**：通过空间分区将网格划分为多个二维网格块，每个处理器负责一个网格块。
- **任务分配（Assignment）**：采用**静态分配（Static Assignment）**，即在执行前确定每个处理器负责的网格块。
- **同步机制（Synchronization）**：
  - 每次计算后需要使用**屏障（Barrier）**来同步所有处理器，以确保所有网格块的数据在进入下一个阶段前都是最新的。
  - 更新共享变量时使用**锁（Locks）**来确保互斥性。

### 2.3 关键工作集（Critical Working Sets）
- 局部单元的**邻域数据**。
- 处理器负责的局部网格块的**三行数据**。

### 2.4 执行时间分析
- 使用 **4D 块布局**可以减少通信时间，因为它提高了数据局部性（块内的计算只依赖块内部的数据只有在处理边界时，才需要与相邻单元交换数据）。
- **同步开销**主要由于屏障等待。

---

## 3. 星系演化模拟（Galaxy Evolution using Barnes-Hut Algorithm）
### 3.1 问题描述
- 将星系表示为 $N$ 个粒子（例如星体），通过引力相互作用来计算星体的运动。
- 使用 **Barnes-Hut 算法**减少计算复杂度：
  - 通过将空间划分为四叉树（Quad-Tree）来表示星系，其中叶节点为星体，内部节点存储子节点的质心和质量。
  - 若质心距离 $D$ 满足 $L/D < \theta$，则可以使用聚合质量计算引力，否则继续递归下探。

### 3.2 工作分配与局部性
- 每个时间步长内构建树结构，计算质心，然后为每个粒子遍历树来累积引力。
- 使用**半静态分配（Semi-static Assignment）**：在每个时间步长中记录每个星体与其他星体的交互次数，用于动态调整任务分配以保持负载均衡。

### 3.3 数据分布与局部性
- **树型结构的局部性**：相邻空间的星体往往具有相似的计算需求，应该将这些星体分配到同一处理器上，以便减少通信。
- **数据分布的挑战**：由于星体分布随时间演化，数据需要动态再分配以保持局部性。

---

## 4. 并行扫描（Parallel Scan）
### 4.1 问题描述
- 对一个数组执行前缀和操作，例如数组 $A = [a_0, a_1, a_2, \dots, a_{n-1}]$，其前缀和为 $[a_0, a_0 \oplus a_1, a_0 \oplus a_1 \oplus a_2, \dots]$，其中 $\oplus$ 为一个关联运算符。

### 4.2 实现方式
- **工作效率低的实现**：
  - 工作复杂度为 $O(N \log N)$，跨度（Span）为 $O(\log N)$。
- **工作高效的实现**：
  - 使用“上升-下降（Up-Sweep and Down-Sweep）”模式，工作复杂度为 $O(N)$，跨度为 $O(\log N)$。

---

## 5. 数据并行分段扫描（Data-parallel Segmented Scan）
### 5.1 问题描述
- 分段扫描是对输入数据集合的各个分段同时执行扫描操作。
- 例如，给定数组 $A = [[1, 2, 3], [4, 5, 6]]$，分段扫描结果为 $[[0, 1, 3], [0, 4, 9]]$。

### 5.2 实现细节
- 使用**头标记（Head-flag）**来区分每个段的起始点，以实现多段的并行扫描。
- 工作复杂度为 $O(N)$，通过合理设计标记和数据访问方式来提高效率。

---

## 6. 光线追踪（Ray Tracing）
### 6.1 问题描述
- 给定一条光线，寻找与场景中几何物体的最近交点。
- 使用**虚拟针孔相机（Virtual Pinhole Camera）**对场景进行投影，每个像素发射一条光线。

### 6.2 并行化策略
- **光线并行性（Ray-Level Parallelism）**：每个核心同时追踪不同的光线，实现“尴尬并行”。
- **SIMD 并行化**：
  - 使用**光线包（Ray Packet）**技术，每个包内包含多条光线，所有光线在场景中的遍历是同步的。
  - 当光线包内的光线变得不一致（例如穿过不同的几何体）时，可能会导致性能下降。

### 6.3 优化策略
- **数据重用**：光线包内的光线共享同一个节点的数据访问，减少 BVH 节点加载的次数。
- **光线重排（Ray Reordering）**：当光线包利用率低时，对光线进行重新排序以提高 SIMD 利用率。

---

## 7. 课程总结
- **案例研究中的关键问题**：
  1. **如何平衡负载？**
  2. **如何利用问题本身的局部性？**
  3. **必要的同步机制是什么？**

- **并行编程的趋势**：
  - 只需要足够的并行度来保持所有处理元素的忙碌（例如，数据并行扫描 vs. 简单多核扫描）。
  - 在不同的工作负载或机器的不同位置上可以应用不同的并行化策略（例如，光线追踪中的光线包 vs. 单光线）。























































- # 第9课：工作负载驱动的性能评估（Workload-Driven Performance Evaluation）

  ---

  ## 1. 课程概述
  - 本节课的主题是如何通过工作负载驱动的方法来进行**并行计算机的性能评估**。
  - 涵盖的内容包括如何衡量性能、选择适当的评估标准、不同的性能度量模型，以及这些模型在评估并行系统时的应用。

  ---

  ## 2. 并行程序性能评估的常用度量
  - **绝对性能（Absolute Performance）**：
    - 通常通过**墙钟时间（Wall Clock Time）**来衡量。
    - 例如，每秒操作数（Operations per Second）。
  - **加速比（Speedup）**：
    - 加速比是由于并行性带来的性能提升：
      $$
      \text{Speedup} = \frac{\text{串行程序的执行时间}}{\text{P 个处理器上的执行时间}}
      $$
    - 或者表示为：
      $$
      \text{Speedup} = \frac{\text{P 个处理器上的每秒操作数}}{\text{串行程序的每秒操作数}}
      $$
  - **效率（Efficiency）**：
    - 以单位资源的性能来衡量。
    - 例如，每个芯片面积、每瓦特、每美元的每秒操作数。

  ### 2.1 性能衡量的注意事项
  - 要避免的常见陷阱是将**并行程序的加速比**与在单个核心上运行的**并行算法**的性能进行比较。正确的做法是使用最优的串行程序作为基准。

  ---

  ## 3. 扩展性（Scaling）的类型
  ### 3.1 固定问题规模的扩展性（Fixed Problem Size Scaling）
  - 在**固定问题规模**下评估性能随处理器数量增加的变化情况。
  - 存在的主要问题：
    - 如果问题规模太小，通信和调度开销会使得**并行开销**大于并行所带来的好处。
    - 如果问题规模太大，单个机器可能会因为内存不足而导致**频繁换页（Thrashing）**，进而严重影响性能。

  ### 3.2 时间约束扩展（Time-Constrained Scaling）
  - 目标是**在固定时间内完成更多的工作**，即随着机器规模的增加，任务的复杂性也增加。
  - 例如，实时 3D 图形渲染中，随着计算能力的增加，可以渲染更复杂的场景。

  ### 3.3 内存约束扩展（Memory-Constrained Scaling）
  - 目标是使用尽可能多的内存来解决最大规模的问题。
  - 在这种扩展下，每个处理器的内存是固定的，但可以通过增加更多的处理器来提高总内存容量。

  ---

  ## 4. 性能扩展模型的案例
  ### 4.1 2D 网格求解器的扩展
  - 对于一个 $N \times N$ 的网格求解器，内存需求为 $O(N^2)$，总工作量为 $O(N^3)$。
  - **问题约束扩展（Problem-Constrained Scaling）**：
    - 固定问题规模 $N$，计算时间与处理器数量成反比。
    - 通信与计算的比例随处理器数量增加而增加，可能导致效率降低。
  - **时间约束扩展（Time-Constrained Scaling）**：
    - 目标是使计算时间固定，扩展网格规模到 $K \times K$，计算时间保持为 $O(N^3)$。
    - 这时，每个处理器的计算量为 $K^2 / P$，通信的需求也随之调整。
  - **内存约束扩展（Memory-Constrained Scaling）**：
    - 扩展网格规模为 $N\sqrt{P} \times N\sqrt{P}$，每个处理器的内存需求保持不变。
    - 这通常可以获得最佳的加速比，因为总工作量和通信的比例随着处理器数量的增加而得到优化。

  ---

  ## 5. 扩展性评估的挑战
  - **复杂的相互关系**：
    - 问题规模的选择会影响负载均衡、通信开销、算术强度以及数据访问的局部性。
    - 这些因素之间的相互作用可能非常复杂，且依赖于应用的具体特性。
    
  - **实际使用场景**：
    - 在评估并行性能时，确保问题规模和机器规模的选择反映出实际的应用场景尤为重要。
    - 例如，如果选择的问题规模太小，评估结果可能无法反映大规模并行计算的真实情况。

  ---

  ## 6. 架构师的扩展性思考
  - **向上扩展（Scaling Up）**：随着核心数量增加，架构的性能如何变化？
    - 架构是否能够扩展到高端系统？
  - **向下扩展（Scaling Down）**：随着核心数量减少，架构的性能如何变化？
    - 架构是否适用于低端系统？
  - **架构的通用性**：
    - 一个并行架构应能够适用于不同规模的系统，从低端到高端都能保持良好的性能表现。

  ---

  ## 7. 案例分析中的扩展性思考
  ### 7.1 时间约束扩展的应用
  - **实时 3D 图形渲染**：例如《刺客信条：大革命》（2014）与《半条命》（1998）的图形效果对比，增加计算能力使得可以渲染更为复杂的场景。
  - **大规模天文观测**：如 LSST（大视场巡天望远镜），通过增加计算能力可以进行更复杂的图像分析。

  ### 7.2 内存约束扩展的应用
  - **大规模机器学习**：例如需要处理数十亿条点击数据或文档。
  - **N 体问题模拟**：例如 2012 年超级计算机 Gordon Bell 奖获奖者使用 K 计算机模拟了 $10^{12}$ 个粒子的 N 体问题。

  ---

  ## 8. 模拟与性能评估
  ### 8.1 架构模拟的作用
  - 架构师通过模拟工具对新的架构特性进行定量评估。
  - 使用详细的模拟器来测试新的特性，但完整地模拟一个并行计算机的成本非常高，因此通常需要缩小工作负载。

  ### 8.2 轨迹驱动模拟（Trace-Driven Simulation）
  - **轨迹记录**：在真实机器上运行代码，记录所有的内存访问轨迹。
  - 使用这些轨迹在模拟器中回放，以评估系统的性能和特性。

  ---

  ## 9. 性能评估中的常见错误
  - **问题规模选择不当**：在小问题上测试并行程序可能无法反映其在大规模并行环境中的表现，尤其是在负载、局部性和通信模式等方面。
  - **不合理的比较基准**：将并行程序的加速比与一个在单核上运行的并行实现相比较可能导致误导性的结果，应该始终与最优的串行实现比较。

  ---

  ## 10. 性能评估的策略
  - **Roofline 模型**：
    - 使用微基准测试来计算机器的峰值性能，并将应用的性能与已知的峰值进行比较，以评估性能瓶颈。
  - **设立高水位线（High Watermarks）**：
    - 判断性能受限于计算、内存带宽还是同步开销？
    - 通过逐步去除不同的瓶颈，例如移除原子操作或锁，来测试性能变化，以识别主导性能的因素。

  ---

  ## 11. 总结
  - **扩展性评估**在并行计算中至关重要。
  - 选择合适的扩展模型（固定问题规模、时间约束、内存约束）可以帮助更好地理解系统的性能特性。
  - **性能评估**需要谨慎选择合适的基准和方法，以避免获得误导性的结果。
  - **经验的重要性**：评估并行程序和并行系统的性能往往需要大量的经验和实践，没有替代经验的捷径。

  











































# 第10课：基于嗅探的缓存一致性（Snooping-Based Cache Coherence）

---

## 1. 课程概述
- 本节课讨论了**缓存一致性问题（Cache Coherence Problem）**，及其在多处理器系统中的重要性。
- 我们主要关注如何通过**嗅探（Snooping）**技术来实现缓存一致性，以及相关协议的工作原理。

---

## 2. 缓存设计回顾
### 2.1 回写（Write-Back）与直写（Write-Through）缓存
- **回写缓存（Write-Back Cache）**：数据更新只写入缓存中，并在替换缓存行时才写回主存。
- **直写缓存（Write-Through Cache）**：每次数据更新都会立即写入缓存和主存。
- **写分配（Write-Allocate）与非写分配（Write-No-Allocate）**：写分配会在写入缓存未命中时将数据行加载到缓存中，而非写分配则不这样做。

---

## 3. 缓存一致性问题
### 3.1 一致性问题的定义
- 在共享内存多处理器系统中，每个处理器都有自己的**本地缓存（Local Cache）**，用于存储共享内存的副本。
- 当不同处理器缓存相同内存位置的不同副本时，可能会导致不同的处理器对该内存位置的值有不同的认知。
  
### 3.2 直观的共享内存期望
- **一致性期望**：对内存地址 $X$ 的读取应该返回由任何处理器最近写入该地址的值。

### 3.3 示例：缓存不一致的情况
- 例如，处理器 $P1$ 写入某内存位置 $X$，然后 $P2$ 读取 $X$ 的值。如果 $P2$ 的缓存中仍然有旧的副本，那么它将读取到旧值，而不是 $P1$ 最新写入的值。

---

## 4. 嗅探缓存一致性方案（Snooping Cache Coherence）
### 4.1 主要思想
- **嗅探协议（Snooping Protocol）**通过将所有相关的缓存操作广播到所有处理器的缓存控制器来维持一致性。
- 每个缓存控制器通过**嗅探（snooping）**系统总线上的内存操作来确定是否需要更新或失效本地缓存中的数据。

### 4.2 简单的嗅探实现
- 假设：
  1. **直写缓存（Write-Through Cache）**。
  2. 一致性的粒度是缓存行。
- 当处理器执行写操作时，缓存控制器会广播一条**失效消息（Invalidation Message）**。
- 其他处理器在读取该缓存行时会因为缓存未命中而从主存中重新加载最新值。

---

## 5. 缓存一致性协议：MSI 协议
### 5.1 MSI 协议状态
- **MSI 协议**用于描述缓存行的三种状态（所有处理器自己的缓存都只能是这三种状态，每次请求根据状态确定是否访问主存）：
  1. **无效（Invalid, I）**：缓存行无效，不能使用。
  2. **共享（Shared, S）**：缓存行可能存在于多个缓存中，数据与内存一致。
  3. **修改（Modified, M）**：缓存行只存在于一个缓存中，且数据已被修改，主存中为旧值。

### 5.2 状态转换规则
- 例如，当缓存行处于共享状态（S）并被写入时，缓存行必须首先通过**总线读独占（BusRdX）**请求变为修改状态（M）。

- **失效广播（Invalidation Broadcast）**：当一个处理器需要独占访问某个缓存行时，它会向所有其他缓存广播一条失效消息，使得其他缓存中相应的缓存行进入无效状态（I）。

- **状态转换**：

  **从 Invalid 到 Shared (I → S)**：当缓存收到读取请求（读空，请求到主存），且该数据在其他缓存中有效时（处理器发起读取请求，这个请求会通过总线广播给其他缓存和主存，他缓存如果有该数据的有效副本，并且状态是 **Modified (M)** 或 **Shared (S)**，则它们会响应请求。如果状态是 **Modified (M)**，持有该数据的缓存会将数据传递给请求方并将其写回主存，确保所有缓存和主存保持一致。在数据被成功传递后，请求方缓存将该数据存储到自己的缓存中，并将其状态设为 **Shared** ）。

  **从 Invalid 到 Modified (I → M)**：当缓存收到写请求（写空，请求到主存）且数据不存在其他有效副本时。

  **从 Shared 到 Modified (S → M)**：当缓存需要对共享数据进行修改时，会升级缓存行。

  **从 Modified 到 Invalid (M → I)**：如果其他处理器请求该数据，则当前缓存需要将其设置为无效并将数据写回内存。

  **从 Modified 到 Shared (M → S)**：当其他处理器请求该数据但不修改时，当前缓存可以提供数据并将状态设置为 Shared。

---

## 6. 写入序列化与传播（Write Serialization and Propagation）
- **写入传播（Write Propagation）**：确保当一个处理器写入某个地址后，所有其他处理器最终都能看到这个写入操作。
- **写入序列化（Write Serialization）**：所有处理器必须以相同的顺序看到对某一内存地址的写入操作。
  - 例如，如果处理器 $P1$ 先写入值 $a$，然后 $P2$ 写入值 $b$，则所有处理器必须先看到 $a$，再看到 $b$。

---

## 7. MESI 协议
### 7.1 引入“独占”状态（Exclusive, E）
- **MESI 协议**在 MSI 的基础上增加了一个状态**独占（Exclusive, E）**：
  - 该状态表示缓存行仅存在于一个缓存中，但数据尚未被修改，因此主存中的值也是最新的。
  - 当缓存行从独占状态变为修改状态时，不需要通过总线进行通信，因为没有其他缓存持有该行的数据。

### 7.2 状态转换
- **从共享状态到修改状态的优化**：在 **Exclusive (E)** 状态下，缓存行未被其他缓存共享，并且与主内存一致。因此，当需要对该数据进行**写操作**时，不需要通知其他处理器或缓存，这意味着不需要额外的通信。

  而在 **MSI 协议**中，如果缓存行处于 **Shared (S)** 状态，任何写操作都必须通知其他缓存，使它们的对应数据变为 **Invalid (I)**，这导致了更多的通信开销

  

---

## 8. 缓存一致性实现中的硬件挑战
### 8.1 硬件实现影响
- 每个缓存必须监听并响应所有在总线上广播的缓存一致性事务。
- 在多核系统中，这种**总线广播**带来的额外通信流量可能会影响可扩展性。

### 8.2 多级缓存层次结构中的一致性
- **三级缓存（L3 Cache）** 是处理器中用于加速数据访问的三级缓存存储器（GPU核心一般只有L1,多个核心共享L2）。现代处理器通常有三级缓存，分别是 L1、L2 和 L3，L3 通常是所有核心共享的缓存级别：
  1. **L1 缓存**：速度最快，容量最小，直接连接到每个核心，用于存储频繁访问的数据和指令。
  2. **L2 缓存**：比 L1 大一些，但速度略慢，也是每个核心专有的，缓存的数据访问比 L1 缓存稍微少一些。
  3. **L3 缓存**：是所有核心共享的，容量最大但速度最慢。它起到统一协调多个核心数据访问的作用，减少内存访问的延迟。
- 现代处理器通常有多级缓存，例如 L1、L2 和 L3 缓存
- **包含性属性（Inclusion Property）**：例如，L1 缓存中的所有缓存行也必须存在于 L2 缓存中。这使得只需 L2 缓存嗅探总线事务即可判断某个缓存行是否在 L1 中。

---

## 9. 假共享（False Sharing）
### 9.1 问题描述
- **假共享**发生在两个处理器对不同的地址（同一个缓存行的不同部分，缓存系统通常以缓存行为单位进行管理，通常每个缓存行大小为 64 字节）进行写操作，但这些地址映射到同一个缓存行中。
- 导致缓存行在两个处理器之间不断“乒乓”，生成大量的通信和失效操作，尽管实际上没有真正的数据共享。

### 9.2 解决方案
- 通过将每个线程的数据独立放置，且保证不同线程的数据不会映射到相同的缓存行，可以有效减少假共享。
- 示例代码：
  ```cpp
  struct PerThreadState {  
      int myPerThreadCounter;  
      char padding[CACHE_LINE_SIZE - sizeof(int)];  
  };
  PerThreadState myPerThreadCounter[NUM_THREADS];

- 在每个线程的计数器后增加填充（Padding），以确保不同线程的计数器位于不同的缓存行中，减少假共享的影响。

------

## 10. 总结

- **缓存一致性问题**源于在多处理器系统中对共享数据的多副本管理。
- **嗅探式缓存一致性协议**通过广播机制维护数据的一致性，典型协议包括 **MSI** 和 **MESI** 协议。
- **硬件实现挑战**包括多核环境中广播流量的增加以及多级缓存中一致性的维护。
- **假共享**是并行编程中需要避免的常见问题，通过适当的数据布局可以减轻其影响。

























































# 第11课：基于目录的缓存一致性

---

## 1. 课程概述
- 本节课讨论了如何通过**基于目录的缓存一致性（Directory-Based Cache Coherence）**来实现大规模并行计算系统中的缓存一致性。
- 主要内容包括：传统嗅探协议的局限性、目录协议的原理、以及如何减少目录的存储开销。

---

## 2. 嗅探式缓存一致性的局限性
- **嗅探协议（Snooping Protocol）**通过广播一致性信息来确定其他缓存中缓存行的状态。
- 在小规模系统中，广播是可行的。然而，随着处理器数量的增加，广播带来的**通信开销**和**总线拥塞**会显著增加，从而限制了系统的可扩展性。
  
### 2.1 扩展性问题
- **非一致内存访问（NUMA, Non-Uniform Memory Access）**系统中，每个处理器与不同的内存位置存在不同的延迟。
- 在**缓存一致性非一致内存访问（cc-NUMA）**系统中，即使访问的是局部内存，仍需广播一致性消息给其他处理器，增加了通信负担。

---

## 3. 基于目录的缓存一致性
- **目录协议（Directory Protocol）**的核心思想是避免广播，通过将每个缓存行的信息集中存储在一个特定的**目录（Directory）**中来实现一致性。
- 每个目录条目包含关于某缓存行在各个缓存中的状态的信息，通过**点对点消息**来维护一致性，而不是广播。

### 3.1 目录的基本结构
- 目录位于内存的**Home 节点**中，每个缓存行的目录条目包含以下信息：
  - **存在位（Presence Bits）**：用于标识某处理器是否持有该缓存行的副本。
  
  - **脏位（Dirty Bit）**：用于标识该缓存行是否已被修改（即是否是最新版本，仅存储在某个处理器缓存中）。
  
    
  
    目录还会存储**持有缓存行的处理器节点的地址**或标识符。这些信息通常用于跟踪哪些处理器缓存了某个内存块的副本，以便在发生数据请求时能准确地与相关缓存进行通信。

### 3.2 目录协议的优势
- **读取操作**：当处理器需要读取某缓存行时，目录直接告知它数据的位置，无论是在内存还是在某个处理器的缓存中。
- **写入操作**：对于写入操作，目录负责通知所有持有该缓存行副本的处理器进行失效操作。

---

## 4. 目录的一致性操作示例
### 4.1 读取缺失到干净行（Read Miss to Clean Line）
- **请求节点**发送读取缺失消息到**Home 节点**。
- 如果缓存行(缓存行存储某些数据的值，缓存行对应位置数据变动代表对应变量值变动)未被修改（脏位为 0），目录从内存中返回数据并将请求节点标记为持有该行。

### 4.2 读取缺失到脏行（Read Miss to Dirty Line）
- **请求节点**发送读取缺失消息到**Home 节点**。
- 如果缓存行已被修改，目录告知请求节点缓存行的**拥有者（Owner）**。
- 请求节点从拥有者处获取数据，并且原有拥有者将缓存行状态修改为共享（Shared）。

### 4.3 写入缺失（Write Miss）
- **请求节点**请求写入某缓存行，该行可能被其他节点缓存为共享状态。
- **Home 节点**通知所有持有该行的节点进行失效，等待所有节点的确认后，允许写入操作。

---

## 5. 减少目录存储开销的方法
### 5.1 完整位向量方案（Full Bit Vector）
- 每个缓存行在目录中有一个完整的位向量，标识所有处理器的存在状态。
- **存储开销**随着处理器数量 $P$ 的增加而显著增加：
  - 假设缓存行大小为 64 字节，$P = 256$ 时，目录开销约为 50%。
  - 如果 $P = 1024$，目录开销甚至可能达到 200%。

### 5.2 有限指针方案（Limited Pointer Scheme）
- 由于在大多数情况下，一个缓存行只被少量处理器共享，因此可以为每个目录条目存储有限数量的指针，指向持有该缓存行的节点。
- 在超过指针容量时，可以选择**回退到广播**，或者**替换旧的持有者**。

### 5.3 稀疏目录（Sparse Directory）
- **稀疏目录**利用了大部分内存行通常不在缓存中的事实，只为当前在缓存中的行维护目录条目。
- 这种方案大大减少了存储开销，但会增加**目录查找和维护的复杂度**。

---

## 6. 目录协议的性能优化
### 6.1 降低存储开销
- 增加缓存行大小以减少目录条目数量。
- 将多个处理器分组为一个节点，减少存在位的数量。
- 使用层次结构（Hierarchical Structure）在组内使用嗅探协议，组间使用目录。

### 6.2 减少通信开销
- 减少点对点通信的消息数量，例如通过**请求转发（Request Forwarding）**和**干预转发（Intervention Forwarding）**优化一致性消息的传递路径，降低关键路径中的延迟。

---

## 7. Intel 系统中的目录一致性实现
### 7.1 Intel Core i7 的目录实现
- **L3 缓存**作为集中式目录，维护所有 L3 缓存行的信息。
- 目录跟踪每个 L2 缓存中是否包含某缓存行，仅向包含该行的 L2 缓存发送一致性消息，而不是广播。

### 7.2 Intel Xeon Phi (Knights Landing) 中的目录一致性
- 采用**二维网格（2D Mesh）**的连接方式，每个 Tile 包含两个核心和 1MB 的 L2 缓存。
- 使用**MESIF 协议**（Modified, Exclusive, Shared, Invalid, Forward）进行缓存一致性管理，其中 "F" 表示可以直接将数据转发给请求者。
- **簇模式（Cluster Modes）**提供不同的目录和内存关系：
  1. **全互连模式（All-to-All）**：地址均匀地分布到所有目录节点，适用于低延迟需求场景。
  2. **象限模式（Quadrant）**：将芯片分为四个虚拟象限，每个象限的目录节点与内存存在紧密关联，从而减少访问延迟。

---

## 8. 总结
- **嗅探协议的局限性**在于其广播机制在扩展到大规模多处理器系统时会带来高通信开销。
- **基于目录的缓存一致性协议**通过集中管理缓存行的状态信息，避免了广播，改善了系统的可扩展性。
- **目录的存储开销**是该协议的主要挑战，可以通过有限指针方案、稀疏目录等方法加以优化。
- Intel 的多核处理器架构中采用了基于目录的缓存一致性管理，结合不同的网络拓扑和协议优化，减少了一致性通信的延迟和开销。



​	









































# 第12课：基于嗅探的多处理器实现

---

## 1. 课程概述
- 本节课介绍了如何在真实系统中实现一个**基于嗅探的失效协议（Invalidation-Based Protocol）**，并讨论了在实现缓存一致性时需要面对的各种挑战。
- 我们重点关注硬件实现中的各种复杂性，以及如何在保持性能和正确性之间取得平衡。

---

## 2. 缓存一致性的实现目标
### 2.1 一致性的目标
1. **正确性**：实现缓存一致性，使所有处理器对内存的读取结果一致。
2. **高性能**：尽可能减少一致性操作的延迟，保证处理器的并行执行。
3. **最小化硬件成本**：减少实现缓存一致性所需的额外硬件量。

### 2.2 挑战
- **高性能与正确性的权衡**：高性能的技术往往使得确保系统正确性变得更加复杂。
- **并行操作**：在并行系统中，多种硬件资源同时运行并竞争共享资源，容易引入**死锁（Deadlock）**、**活锁（Livelock）**和**饥饿（Starvation）**等问题。

---

## 3. 一致性协议的状态转换：MESI 协议
- **MESI 协议**是实现缓存一致性的常见协议，其缓存行有四种可能的状态：
  1. **Modified (M)**：缓存行已被修改，且该行只存在于一个缓存中。
  2. **Exclusive (E)**：缓存行与主存一致，但只存在于一个缓存中。
  3. **Shared (S)**：缓存行可能存在于多个缓存中，且与主存一致。
  4. **Invalid (I)**：缓存行无效。

### 3.1 MESI 状态转换示例
- **写入操作**：当一个处理器要对处于共享（S）状态的缓存行进行写入时，需要先通过总线请求独占权限，将缓存行状态从共享变为修改（M）。
- **失效机制**：当一个处理器请求对某缓存行的独占访问时，所有其他处理器必须将该缓存行标记为无效（I）。

---

## 4. 死锁、活锁与饥饿
### 4.1 死锁（Deadlock）
- **定义**：系统中有若干未完成的操作，但由于互相竞争共享资源而无法继续前进。
- **例子**：每个操作都持有其他操作所需的资源，从而形成了一个**循环依赖**，导致整个系统停滞。

### 4.2 活锁（Livelock）
- **定义**：系统不断执行操作，但没有任何线程取得有意义的进展。

- **例子**：两个处理器反复请求相同的缓存行的独占访问权限，但每次在写入之前（写操作需要独占缓存行，一个处理器必须使其他处理器的缓存副本失效，才能进行写入，）即在其他缓存回应请求前，另一个处理器获得了总线并请求独占权限导致之前独占失效无法写入，导致双方互相失效对方的数据。

- **解决方案**：

  **随机退避（Random Backoff）**：

  - 当处理器请求独占访问权限失败时，引入一个**随机延迟**，避免两个处理器在相同的时间段再次尝试请求，打破不断互相竞争的局面。

  **优先级机制（Priority Mechanism）**：

  - 引入一个**优先级机制**，比如让某个处理器在争用资源时具有优先权，直到完成它的操作为止，从而防止两个处理器不断互相抢占独占权限。

  **队列调度**：

  - 可以在总线上实现**请求队列**，确保每次有且仅有一个处理器获得独占权限，同时根据公平的调度策略，轮流给每个请求方独占权限。

  **缓存一致性协议的改进**：

  - 改进缓存一致性协议，减少对总线请求的反复竞争。例如，使用**MESI** 这样的协议增加状态来优化不同请求的处理过程，避免频繁互相失效的情况。

### 4.3 饥饿（Starvation）
- **定义**：系统整体在进行进展，但某些进程由于资源竞争而一直无法得到执行机会。
- **例子**：某些处理器持续无法获得总线访问权限，导致它们的请求一直得不到满足。

---

## 5. 基本的嗅探实现
- **嗅探总线**：系统中所有缓存控制器都监听总线上广播的内存事务，以确保它们的缓存行状态与其他处理器的操作保持一致。
- **原子总线事务**：假设系统中只有一个处理器能够在某一时刻控制总线，确保每次总线请求都是原子的。
  
### 5.1 基本设计要点
- 每个处理器有一个**回写缓存（Write-Back Cache）**：

  处理器使用回写缓存来减少主内存的写入次数。当缓存中的数据被修改时，数据只保存在缓存中，等到被替换或者失效时才写回内存，从而减少了对主内存的频繁访问

- 每个缓存可以**暂停当前处理器**，以便执行缓存一致性操作：

  为了处理缓存一致性请求（例如使其他缓存中的共享缓存行失效），缓存需要暂时暂停当前处理器的操作（防止数据在修改之前被处理器访问）。这保证了缓存一致性协议可以在缓存数据被修改之前完成所需的状态更新，确保所有缓存副本的一致性

- **系统互连（System Interconnect）**是一个共享的原子总线（一次只有一个缓存可以通信）：

  共享的原子总线意味着每次只有一个缓存控制对总线的访问。这样可以确保任何缓存一致性操作都是原子性的，防止多个处理器同时访问和修改总线上的数据。这有助于避免冲突，保证一致性协议的有效执行

### 5.2 缓存控制器行为
- 当总线需要访问缓存控制器的标签（Tag）时，处理器会被**锁定**，无法访问自己的缓存。
- 当处理器优先访问缓存时，缓存不能同时执行嗅探操作，这会导致其他处理器的请求被延迟。

---

## 6. 减少资源竞争的优化
### 6.1 允许同时访问
- 为了减少处理器和嗅探控制器对缓存的**竞争**，可以考虑以下两种方案：
  1. **缓存标签复制（Duplicate Tags）**：缓存中维护一份标签的副本，处理器和嗅探控制器可以同时访问不同的标签副本。
  2. **多端口标签存储（Multi-Ported Tag Memory）**：缓存标签存储有多个端口，允许同时进行多个访问操作。

### 6.2 写回缓冲区（Write-Back Buffer）
- 当缓存行被逐出并且需要写回内存时，使用写回缓冲区可以让处理器在缓存行写回之前就继续执行其他操作（当缓存中的数据被修改时，数据只保存在缓存中，等到被替换或者失效时才写回内存，从而减少了对主内存的频繁访问）。
- 如果总线上出现请求写回缓冲区中的地址，嗅探控制器必须检查写回缓冲区，并返回正确的数据，而不是缓存中的数据。

---

## 7. 非原子总线事务及其实现
### 7.1 非原子事务的动机
- **原子总线事务的限制**：当一个请求在等待响应时，总线会保持空闲状态，导致总线带宽利用率低。
- **分割事务（Split-Transaction Bus）**：将总线事务拆分为请求和响应两个独立的部分，这样其他事务可以在请求和响应之间进行，从而提高总线的利用率。

### 7.2 分割事务带来的新问题
- **请求与响应匹配**：需要能够跟踪多个并发请求和响应，确保每个响应能正确地对应到最初的请求。
- **流量控制（Flow Control）**：需要控制总线上请求的数量，以避免请求过多导致缓冲区溢出。

---

## 8. 总结
- **基于嗅探的缓存一致性实现**在并行系统中至关重要，但其实现需要解决多个复杂的并发问题。
- **死锁、活锁和饥饿**是实现一致性时需要特别注意的挑战，通过合理的设计可以避免这些问题。
- **优化方法**包括允许嗅探控制器和处理器同时访问缓存、使用写回缓冲区以及采用分割事务以提高总线带宽的利用效率。
  













































# 第13课：内存一致性模型（Memory Consistency Models）

---

## 1. 课程概述
- 本节课讨论了**内存一致性（Memory Consistency）**和**内存一致性模型（Memory Consistency Models）**，并探讨了不同的内存操作顺序如何影响并行程序的执行结果。
- 通过本课程，理解**松弛一致性模型（Relaxed Consistency Models）**背后的动机和其对内存操作排序的影响。

---

## 2. 内存一致性与内存一致性模型
### 2.1 内存一致性 vs. 缓存一致性
- **缓存一致性（Memory Coherence）**：
  - 定义了对同一内存地址的读写操作的可见性要求。
  - **所有处理器必须一致地观察到某个内存地址的写入顺序**。
- **内存一致性（Memory Consistency）**：
  - 定义了对**不同内存地址**的读写操作在不同处理器之间的行为。
  - 缓存一致性保证写入某个地址的数据最终会传播到所有处理器，但内存一致性则关心这些写操作相对于对其他地址的读写的传播时间。

### 2.2 示例：内存一致性 vs 缓存一致性
- 缓存一致性确保对于某一地址 $X$，所有处理器按相同的顺序观察写入操作。
- 内存一致性则描述了在有多个地址的情况下，处理器如何看到这些地址的读写操作。

---

## 3. 内存操作的顺序（Memory Operation Ordering）
### 3.1 四种内存操作顺序
- **W→R**：写操作必须在后续的读操作之前提交。
- **R→R**：读操作必须在后续的读操作之前提交（读取的值与程序中指定的顺序相一致，防止后续的读取操作得到旧值或者未被更新的值）。
- **R→W**：读操作必须在后续的写操作之前提交。
- **W→W**：写操作必须在后续的写操作之前提交。
  
- **顺序一致性（Sequential Consistency）**：
  - 在顺序一致性模型中，所有处理器的内存操作按照**程序顺序（Program Order）**执行。
  - 并行系统的结果与某种**顺序执行**所有内存操作的情况相同。
  
  举例：处理器A 执行x=1，并同时传给其他请求x的处理器B,C，但由于x的值先传到处理器B(更近)，B在x=1时会让y=1,并传递给请求y的C，C在y=1时复制z=x，此时A中的x值传递由于延迟还没到，就会导致C读取的A是脏数据（即正确顺序应当是x=1,y=1,z=x但对于C的顺序是y=1,z=x,x=1，导致了非顺序执行，主要原因是延迟，所以必须保证顺序一致性）

### 3.2 顺序一致性的示例
- 处理器按顺序执行每个存储操作，选择某个处理器，完成其存储操作，再选择下一个处理器，依次类推，直到所有操作完成。

---

## 4. 放松内存一致性模型（Relaxed Memory Consistency Models）
### 4.1 放松内存操作顺序的动机
- **隐藏延迟**：
  - 放松内存操作顺序的一个主要动机是为了提高性能，通过允许一些操作在其他操作之前执行，以**隐藏内存访问的延迟**。
  - 例如，在某些情况下，处理器可以在写操作之前进行读操作，从而加速程序执行。

### 4.2 放松内存操作顺序的类型
- **总存储顺序（Total Store Ordering, TSO）**：
  - 处理器可以在写入操作之前读取**其他地址**（只需要保证具有依赖关系的操作的先后顺序不被打破就可，其他没有依赖的可以不受影响），但其他处理器在观察到写入之前，不能读取新的值。
- **处理器一致性（Processor Consistency, PC）**：
  - 任何处理器都可以在写入操作之前读取其他地址。

- **写缓冲优化（Write Buffering）**：
  - 写缓冲区可以让处理器在等待写操作完成之前开始执行随后的读操作。这种机制**放松了 W→R 顺序**，允许读操作优先于等待中的写操作。

### 4.3 例子：不同一致性模型的表现
- **四个程序的示例**：
  - 假设两个线程分别对变量 A 和 B 进行读写操作，程序的行为会根据一致性模型的不同而有不同的结果。例如，在顺序一致性模型下，线程之间的操作严格按照时间顺序进行，而在松弛一致性模型下，处理器可以重新排序某些操作（不具有依赖性的操作）以提高效率。

---

## 5. 允许内存操作重新排序的其他模型
### 5.1 部分存储顺序（Partial Store Ordering, PSO）
- **PSO**模型允许处理器重新排序写操作（W→W），例如一个写操作是缓存命中，另一个是缓存缺失，这样的重新排序可以减少等待时间。
  
### 5.2 弱排序（Weak Ordering, WO）和释放一致性（Release Consistency, RC）
- **弱排序**和**释放一致性**允许在读取和写入之间进行更多的重新排序，以最大化并行性。
- **内存栅栏指令（Memory Fence）**：
  - 通过插入**内存栅栏（Memory Fence）**指令来防止某些操作被重新排序。
  - 例如，在 **x86/x64** 中可以使用 `mm_lfence`、`mm_sfence` 和 `mm_mfence` 来确保某些内存操作的顺序。

---

## 6. C++11 中的内存模型
### 6.1 `atomic<T>` 的使用
- 在 C++11 中，提供了 **atomic** 类型用于实现**原子性读写（Atomicity）**和**读-改-写（Read-Modify-Write）**操作。
- 例如：
  ```cpp
  atomic<int> ready;
  int foo;
  
  foo = 1;
  ready.store(1, memory_order_release);
  
  // 其他代码
  while (ready.load(memory_order_acquire) == 0);
  // 使用 foo

- 获取/释放语义（Acquire/Release Semantics）

  ：

  - **获取（Acquire）**语义：确保在获取操作之后的所有读取和写入在程序顺序中保持不变。
  - **释放（Release）**语义：确保在释放操作之前的所有操作在程序顺序中保持不变。

### 6.2 数据竞争（Data Race）

- **数据竞争**发生在多个处理器对同一个内存地址进行访问，且至少一个为写操作，且没有使用同步机制对这些访问进行排序。
- 通过原子操作和内存栅栏可以避免数据竞争，并确保程序在松弛一致性模型下的行为与顺序一致性模型一致。

------

## 7. 总结：放松一致性模型的优缺点

- 优点：

  - **性能提升**：通过允许某些内存操作的重新排序，可以隐藏内存访问的延迟，从而提高系统的整体性能。

- 缺点：

  - **软件复杂性**：编写正确的并行程序变得更加复杂，因为程序员需要确保必要的同步和内存栅栏来维持正确性。
- 在实践中，复杂性通常被封装在库中，例如使用锁、屏障等同步原语。

------

## 8. 分布式系统中的最终一致性（Eventual Consistency）

- **最终一致性**是许多分布式系统采用的模型，它保证在没有新的更新发生时，所有节点最终会达到一致状态。
- 例如，在社交媒体中，多个数据库的副本可能在一段时间内不一致，但最终会达到一致。















































# 第14课：网站扩展（Scaling a Web Site）

---

## 1. 课程概述
- 本节课讨论了如何通过并行化、弹性扩展和缓存等技术扩展一个网站以应对用户增长和负载波动。
- 核心概念包括**扩展性（Scalability）**、**弹性（Elasticity）**和**缓存（Caching）**。

---

## 2. 网站扩展中的并行性
### 2.1 简单 Web 服务器的实现
- 基本的 Web 服务器设计如下：
  ```c
  while (1) {
    request = wait_for_request();
    filename = parse_request(request);
    contents = read_file(filename);
    send contents as response;
  }

- 问题：网站性能是由**吞吐量（Throughput）**还是**延迟（Latency）**决定的？

### 2.2 多进程 Web 服务器

- 为了提高吞吐量，可以使用**多个工作进程（Worker Processes）**来并行处理请求。
- 每个工作进程处理请求的步骤相同，父进程动态管理工作进程的数量。
- 设定工作进程数量 N 的考虑因素：
  - **并行度（Parallelism）**：使用所有服务器核心。
  - **隐藏延迟（Latency Hiding）**：通过在工作进程之间进行上下文切换来隐藏长延迟磁盘读操作。
  - **并发性（Concurrency）**：处理多个未完成的请求。
  - **工作集大小（Footprint）**：避免创建过多线程，以防造成内存抖动（Thrashing）。

### 2.3 Apache 的工作进程池管理

- **Apache 服务器**通过父进程动态管理工作进程池的大小。
- 保持一些空闲的工作进程，避免在请求服务的关键路径中进行进程创建。
- 限制最大工作进程数量以避免过高的内存使用。

### 2.4 进程 vs 线程

- 为什么将服务器划分为多个进程而不是线程？
  - **保护性（Protection）**：不希望一个工作进程崩溃时影响整个服务器。
  - 使用**非线程安全库**时，进程的隔离比线程更安全。
  - 父进程可以定期回收工作进程，以应对内存泄漏。

------

## 3. 动态 Web 内容

- 对于动态内容，Web 服务器不仅从磁盘上读取静态文件，还需要**应用逻辑**来生成内容（例如通过数据库查询）。
- 例如，生成 Facebook 新闻源可能涉及复杂的逻辑和多个数据库查询。

### 3.1 脚本语言的性能问题

- 使用脚本语言（如 PHP、Ruby、Python）的性能通常较差：
  - **WordPress**：每核大约支持 12 个请求/秒。
  - **MediaWiki**：每核大约支持 8 个请求/秒。
- 为了解决性能问题，采用了以下措施：
  - **Facebook HipHop**：PHP 转换为 C 的源代码转换器。
  - **Google V8**：JavaScript 的即时编译（JIT）引擎。

------

## 4. 扩展服务器——“Scale Out”

### 4.1 使用负载均衡器扩展

- 通过增加多个 Web 服务器来达到所需的吞吐量目标。
- **负载均衡器（Load Balancer）**维护可用 Web 服务器列表，并估算每个服务器的负载。
- 分发请求到服务器池，负载均衡逻辑通常很便宜。

### 4.2 持久性负载均衡（Load Balancing with Persistence）

- 会话关联性（Session Affinity，或“粘性会话”）：同一会话的所有请求都分配到相同的服务器。
  - **优点**：不需要更改 Web 应用程序设计来实现扩展。
  - **缺点**：有状态的服务器可能会限制负载均衡选项；如果服务器崩溃，会话将丢失。

### 4.3 无状态服务器

- **无状态服务器（Stateless Servers）**：将会话作为持久化数据存储在数据库中，这样可以避免会话丢失，并简化扩展管理。

------

## 5. 数据库扩展与争用问题

### 5.1 纵向扩展数据库（Scale Up Database）

- 增加硬件资源

  来扩展数据库（例如购买更强大的服务器）。

  - **优点**：无需改变软件。
  - **缺点**：成本高且扩展有限。

### 5.2 水平扩展数据库——复制与分区

- **复制（Replication）**：将数据库副本用于并行读取，减轻主数据库的负载，但会增加一致性问题。
- **分区（Partition）**：根据数据访问模式，将数据库按用户或数据类型划分，例如将用户数据按字母顺序划分，便于针对不同工作负载进行调优。

------

## 6. 弹性扩展（Elastic Scaling）

### 6.1 网站流量的突发性（Burstiness）

- 网站流量通常具有突发性，例如节假日的购物季、重大事件后的社交媒体热潮等。
- 问题：
  - 按平均负载进行资源配置可能导致高峰期间服务质量低下或失败。
  - 按高峰负载配置则会导致大多数时间服务器闲置，浪费成本。

### 6.2 弹性计算的概念

- 弹性（Elasticity）

  ：根据监测到的负载自动增加或减少服务器。

  - 需要随时可用的服务器资源，例如**Amazon EC2**、**Google Cloud Platform**、**Microsoft Azure**。

### 6.3 例子：Amazon EC2

- **Amazon EC2** 通过将未被利用的计算资源出租给其他用户，解决了他们自己过度配置的资源浪费问题。

------

## 7. 缓存（Caching）提高性能

### 7.1 数据库与服务器负载的缓存

- 缓存常用对象

  可以减少数据库的查询次数，并减少 Web 服务器在请求之间的数据传递。

  - 例如使用 **Memcached** 作为内存中的键值存储，缓存查询结果或中间结果。

### 7.2 Facebook 缓存部署示例

- 在 2008 年，Facebook 使用了 800 台 **Memcached** 服务器，总计 28 TB 的缓存数据。
- 每秒处理 200,000 个 UDP 请求，延迟为 173 毫秒。

### 7.3 使用内容分发网络（CDN）进行缓存

- 对于

  大型媒体文件

  （如图片、视频），可以通过**内容分发网络（Content Distribution Network，CDN）**来减少带宽和服务器负载。

  - 物理上靠近用户的数据中心提供更高的带宽和更低的延迟。

------

## 8. 网站扩展的总结

- 并行性：
  - 通过扩展 Web 服务器数量来应对吞吐量需求（水平扩展）。
  - 通过弹性扩展来应对突发流量。
  - 数据库扩展通常需要复制、分区以及权衡一致性。
- 本地性与重用：
  - 缓存一切可以缓存的内容：包括数据库查询结果、请求处理结果等。
  - **CDN** 用于缓存大文件内容，靠近用户，减少访问延迟。
- 专门化的实现：
  - 针对不同的请求和工作负载模式进行专门化优化，例如为不同类型的请求选择不同的数据库。

------

## 9. 结语

- 扩展现代网站不仅仅是简单的脚本开发，而是一个复杂的并行系统问题，涉及并行性、负载均衡、数据一致性等多个方面的优化。
- 本课程中的许多并行优化技术都在网站扩展中有所应用，如**工作负载平衡**、**数据复制与争用**、**延迟与吞吐量的权衡**等。

















































# 第15课：互连网络（Interconnection Networks）

---

## 1. 课程概述
- 本节课讨论了并行计算中处理器、缓存和内存之间的互连网络设计。
- 互连网络的设计对系统的扩展性、性能和能效至关重要。
- 本课将探讨不同的互连拓扑、路由、流控制以及性能权衡。

---

## 2. 互连网络的用途
- **互连网络**主要用于连接以下组件：
  - **处理器核心之间**。
  - **处理器与内存**之间。
  - **处理器核心与缓存**之间。
  - **缓存与缓存**之间。
  - **输入/输出（I/O）设备**。

### 2.1 设计互连网络的重要性
- **系统扩展性（Scalability）**：
  - 系统规模可以有多大？
  - 如何方便地增加更多节点（如更多的核心）？
- **系统性能与能效**：
  - 核心、缓存和内存之间通信的速度。
  - 访问内存的延迟。
  - 通信过程中能耗的大小。

---

## 3. 互连网络的基本术语
- **网络节点（Network Node）**：与路由器/交换机连接的网络端点，例如处理器缓存、内存控制器。
- **网络接口（Network Interface）**：连接节点与网络。
- **交换机/路由器（Switch/Router）**：连接输入和输出链路。
- **链路（Link）**：传递信号的一组导线。

---

## 4. 设计问题
- **拓扑（Topology）**：交换机通过链路如何相互连接，影响路由、吞吐量、延迟和实现的复杂度与成本。
- **路由（Routing）**：消息在网络中如何从源到达目的地，可能是**静态路由**（预定路径）或**基于负载的自适应路由**。
- **缓冲与流控制（Buffering and Flow Control）**：
  - 网络中存储的数据是什么（如数据包、部分数据包）。
  - 网络如何管理缓冲区空间。

### 4.1 互连拓扑的特性
- **路由距离（Routing Distance）**：节点之间的链路数量，即沿路径的“跳数”。
- **直径（Diameter）**：所有可能路径中最大的路由距离。
- **平均距离（Average Distance）**：所有有效路由的平均距离。

---

## 5. 互连网络拓扑类型

通过设计网络结构以应对不同组织形式中的通信距离，经可能降低通信的平均距离

- **总线（Bus）**：
  - **优点**：设计简单、成本低，适用于少量节点。易于通过嗅探（snooping）实现一致性。
  - **缺点**：所有节点争用共享总线，带宽受限，节点增加时性能下降，存在电气负载高的问题。

- **交叉开关（Crossbar）**：
  - **优点**：所有节点之间直接连接，具有 $O(1)$ 的延迟和高带宽。
  - **缺点**：不具备扩展性，硬件开销为 $O(N^2)$，且成本高。

- **环（Ring）**：
  - **优点**：设计简单，成本为 $O(N)$。
  - **缺点**：延迟为 $O(N)$，随着节点增加，带宽保持不变，影响扩展性。

- **网格（Mesh）**：
  - **优点**：直连网络，具有本地化特性，容易在芯片上布局，路径多样。
  - **缺点**：平均延迟为 $O(\sqrt{N})$，对网络大小敏感，成本为 $O(N)$。

- **环面（Torus）**：
  - **优点**：解决了网格中边缘节点和中心节点的性能差异问题，提供更高的路径多样性和较大的二分带宽。
  - **缺点**：更高的复杂度，芯片布局较难，链路长度不均匀。

- **树（Tree）**：
  - **优点**：适用于具有局部性流量的应用，延迟为 $O(\log N)$，具有***二分带宽性***
  
    (***\*二分带宽是指将整个网络分成两半，并测量这两部分之间\**最大可能的数据传输速率**。它代表网络在最不利分割情况下，仍能维持的总带宽。高二分带宽意味着网络在进行大量并行通信时，具有良好的性能和低延迟，这对超算集群和并行处理系统尤为重要)。
  
  - **缺点**：根节点带宽成为瓶颈，使用**胖树（Fat Tree）**可以缓解该问题。
  
- **多级对数型（Multi-Stage Logarithmic）**：
  - **特点**：间接网络，多个交换机用于连接端点，延迟为 $O(\log N)$，如**Omega网络**和**Butterfly网络**。

---

## 6. 流控制与传输粒度
- **消息（Message）**：网络客户端之间的传输单元，可以用多个数据包传输。
- **数据包（Packet）**：网络的传输单元，可以用多个“流控位”（flits）来传输。
- **流控位（Flit，Flow Control Digit）**：数据包被分解为多个较小的单位，这些单位是路由和缓冲的最小粒度。

### 6.1 包格式
- **头部（Header）**：包含路由和控制信息。
- **有效载荷/主体（Payload/Body）**：包含要传输的数据。
- **尾部（Tail）**：包含控制信息，例如错误校验码。

### 6.2 争用处理
- 当两个数据包需要同时传输到相同链路时，处理选项包括：
  - 缓冲其中一个数据包，稍后再传输。
  - 丢弃一个数据包。
  - 重路由一个数据包（偏转）。

---

## 7. 路由与流控方法
- **电路交换（Circuit Switching）**：发送消息前预先建立从发送方到接收方的完整路径。
  - **优点**：带宽高，传输期间没有链路管理的开销。
  - **缺点**：需要为路径设置和释放进行额外的开销，低链路利用率。

- **数据包交换（Packet Switching）**：为每个数据包单独进行路由决策。
  - **优点**：每当链路空闲时可以用于其他数据包。
  - **缺点**：传输中存在动态切换逻辑的开销。

### 7.1 具体的流控方式
- **存储转发（Store-and-Forward）**：整个数据包在移动到下一个节点之前，需完全存储在当前交换机中。
  - **缺点**：高延迟，需在每个交换机存储完整的数据包。

- **剪切式流控（Cut-Through Flow Control）**：交换机在接收到数据包的头部后立即开始传输到下一链路，减少了传输延迟。
  - **在高争用时**，退化为存储转发。

- **虫孔流控（Wormhole Flow Control）**：数据包被分为多个 flit，头 flit 带有路由信息，后续 flit 紧随其后。
  - **优点**：可以实现完全流水线化的传输，尤其对于长消息，其延迟与网络距离基本无关。

---

## 8. 当前研究课题与挑战
- **互连的能效**：互连网络是高能耗的组成部分，占据芯片总功耗的显著比例。
- **流量优先级和服务质量**：
  - 优化多处理器系统中对不同应用的网络性能，例如对某些延迟敏感的应用提供更高优先级。
- **新兴互连技术**：
  - **3D 芯片堆叠（Die Stacking）**。
  - **片上光网络（Photonic Networks-on-Chip）**。
  - **可重构设备（如 FPGA）**：实现专用互连结构以满足特定应用的需求。

---

## 9. 总结
- 现代多处理器系统中的互连网络性能对系统整体性能至关重要。
- 拓扑、流控、路由选择等不同设计直接影响互连网络的扩展性、性能和能耗。
- 学习并理解不同互连拓扑的优缺点（如总线、交叉开关、网格、环面、树、超立方体等）有助于设计适应不同工作负载的高性能互连网络。























































# 第16课：同步实现（Synchronization Implementation）

---

## 1. 课程概述
- 本节课讨论了如何在并行计算中**有效地实现同步**，重点是互斥和事件信号等同步原语。
- 涉及到的主题包括**锁（Locks）**、**原子操作（Atomic Primitives）**、**屏障（Barriers）**等。

---

## 2. 同步原语
### 2.1 互斥确保（Mutual Exclusion）
- **锁（Locks）**：
  
  - 用于确保多个线程不会同时进入关键区。

  - 包括简单的**Test-and-Set 锁**和改进的**Test-and-Test-and-Set 锁**。
  
    原理：在汇编层面使用**TS（Test and Set）** 和 **ST（Set and Test）**这两个原子操作实现对变量值的判断来确定是否上锁，以及解锁等操作
  
- **原子原语（Atomic Primitives）**：
  
  - 常用的原子操作包括 `atomic_add`、`atomic_compare_and_swap`（CAS）等。
  
- **事务（Transactions）**：
  - 在后续课程中讨论，事务可以实现对临界区的乐观控制，通过**回滚机制（Rollback Mechanism）**保证正确性。

### 2.2 事件信号
- **屏障（Barriers）**：
  - 用于同步线程，在所有线程达到屏障之前，任何线程都不能继续。

---

## 3. 同步事件的三个阶段
1. **获取方法（Acquire Method）**：
   - 描述线程如何尝试获得对受保护资源的访问权限。

2. **等待算法（Waiting Algorithm）**：
   - 描述线程在等待访问共享资源时的行为。

3. **释放方法（Release Method）**：
   - 线程在完成同步区域的工作后，如何让其他线程获得该资源。

---

## 4. 忙等待与阻塞
### 4.1 忙等待（Busy Waiting）
- **忙等待（Spinning）**：通过循环等待条件满足的方式实现同步。
  ```c
  while (condition X not true) { }

- 在课程如 15-213 或操作系统中，通常被教导**忙等待是低效的**，因为它浪费了处理器资源。

### 4.2 阻塞同步（Blocking Synchronization）

- 当资源不可用时，线程会主动让出执行资源以便其他线程可以使用处理器。

  ```
  c复制代码if (condition X not true)
      block until true;  // 操作系统调度器取消线程的调度，允许其他线程使用处理器
  ```

### 4.3 忙等待 vs. 阻塞

- **忙等待**适用于预计等待时间短、调度开销大的场景。
- 如果系统没有超载，并且没有多个 CPU 密集型程序同时运行，则忙等待可能是合适的。

------

## 5. 实现锁

### 5.1 简单但不正确的锁实现

- 使用**加载-测试-存储（LOAD-TEST-STORE）**实现锁的简单方式存在数据竞争问题，因为该过程不是原子的。

### 5.2 Test-and-Set 锁

- Test-and-Set 指令

  用于原子地检查内存值并设置：

  ```
  ts R0, mem[addr]   // 如果 mem[addr] 为 0，则设置 mem[addr] 为 1
  ```
  
  - **优点**：简单，低存储成本。
  - **缺点**：会不断地轮询某个内存位置的值，看看它是否可以修改（即判断锁是否已被释放），从而会产生大量的**一致性流量（Coherence Traffic）**（在多处理器系统中维持各个处理器缓存之间的数据一致性而产生的通信流量），随着等待处理器数量增加，性能急剧下降。

### 5.3 Test-and-Test-and-Set 锁

- 在 Test-and-Set 基础上增加额外的检查步骤，以减少总线上的一致性流量。

  ```
  void Lock(int* lock) {
      while (1) {
          while (*lock != 0);  // 忙等待直到锁释放
          if (test_and_set(*lock) == 0) return;  // 获取锁
      }
  }
  ```

  - **优点**：在等待期间主要在本地缓存中进行检查（lock值存储在本地缓存），减少了一致性流量。
  - **缺点**：在无争用情况下延迟略高。

### 5.4 带退避的 Test-and-Set 锁（Back-off）

- 在失败后等待一段时间再重试，以减少锁的争用和总线流量。

  ```
  c复制代码void Lock(volatile int* l) {
      int amount = 1;
      while (1) {
          if (test_and_set(*l) == 0) return;
          delay(amount);
          amount *= 2;  // 指数级退避
      }
  }
  ```

  - **缺点**：退避机制可能导致严重的**不公平性**，新请求可能因退避时间较短而比先前请求更快获得锁。

### 5.5 票据锁（Ticket Lock）

- 每个线程获取一个“票据”，锁按票据顺序授予：

  ```
  c复制代码struct lock {
      int next_ticket;
      int now_serving;
  };
  
  void Lock(lock* l) {
      int my_ticket = atomic_increment(&l->next_ticket);  // 获取票据
      while (my_ticket != l->now_serving);  // 等待轮到自己
  }
  
  void Unlock(lock* l) {
      l->now_serving++;
  }
  ```

  - **优点**：只有一个无效操作（invalidation）用于锁释放，通信流量为 $O(P)$，更具扩展性。
  - **缺点**：需要顺序等待，但避免了不公平性。

### 5.6 基于数组的锁（Array-Based Lock）

- 每个处理器在不同的内存地址上进行自旋，减少了锁释放时的一致性流量：

  ```
  c复制代码struct lock {
      padded_int status[P];  // 每个处理器一个状态位
      int head;
  };
  int my_element;
  
  void Lock(lock* l) {
      my_element = atomic_circ_increment(&l->head);
      while (l->status[my_element] == 1);
  }
  
  void Unlock(lock* l) {
      l->status[my_element] = 1;
      l->status[circ_next(my_element)] = 0;
  }
  ```

  - **优点**：锁释放时的通信流量为 $O(1)$。
  - **缺点**：需要线性于处理器数量的存储空间。

------

## 6. 实现屏障

### 6.1 集中式屏障（Centralized Barrier）

- 集中式屏障

  使用一个共享计数器，所有线程到达屏障时同步：

  ```
  c复制代码struct Barrier_t {
      LOCK lock;
      int counter;  // 初始化为 0
      int flag;
  };
  
  void Barrier(Barrier_t* b, int p) {
      lock(b->lock);
      int num_arrived = ++(b->counter);
      unlock(b->lock);
  
      if (num_arrived == p) {
          b->counter = 0;
          b->flag = 1;  // 最后一个到达者设置标志
      } else {
          while (b->flag == 0);  // 等待所有线程到达
      }
  }
  ```

  - **缺点**：在共享锁上存在序列化瓶颈，导致延迟为 $O(P)$。

### 6.2 合并树屏障（Combining Tree Barrier）

- 合并树屏障

  利用树结构将等待过程并行化，减少了瓶颈：

  - **优势**：通过合并树结构，每个线程只需要与其父节点通信，最后由根节点通知所有子节点，减少了同步延迟到 $O(\log P)$。

------

## 7. C++11 中的原子操作

- 提供了

  原子对象（atomic<T>）

  ，实现基本对象的原子读、写和读-改-写操作：

  ```
  cpp复制代码atomic<int> i;
  i++;  // 原子地增加 i
  i.compare_exchange_strong(a, 10);  // 如果 i 等于 a，则将 i 设置为 10
  ```

- 提供了默认的**顺序一致性（Sequential Consistency）**，确保所有线程对原子变量的操作具有相同的顺序视图。

## 8. 总结

- 本节课详细探讨了如何实现并行系统中的同步原语，包括锁的不同实现方式、屏障的实现和优化等。
- 通过理解这些同步方法及其优缺点，可以为设计并行算法时选择合适的同步机制提供指导。



















































# 第17课：细粒度同步与无锁编程

---

## 1. 课程概述
- 本节课重点讲解**并发数据结构**的实现，包括**细粒度锁定（Fine-Grained Locking）**和**无锁编程（Lock-Free Programming）**。
- 涉及如何通过减少锁的使用来实现高效的并行数据结构操作，并确保在不使用锁的情况下实现线程安全。

---

## 2. 基本概念：互斥与无锁
- **互斥锁（Locks）**：通过锁来确保多个线程不会同时访问共享数据，以避免竞态条件。
- **无锁编程（Lock-Free Programming）**：确保没有线程会因某个线程挂起而无限期等待。无锁算法通过原子操作（如`compare_and_swap`）保证系统整体的进展。

---

## 3. 细粒度同步的锁实现
### 3.1 用 `compare_and_swap` 实现的锁
- `compare_and_swap`（CAS）是一种原子操作，能够比较内存中的值，如果相等则交换值。
- 利用 CAS 可以实现简单的锁机制：
  ```cpp
  typedef int Lock;
  
  void lock(Lock* l) {
      while (atomicCAS(l, 0, 1) == 1);
  }
  
  void unlock(Lock* l) {
      *l = 0;
  }

- CAS 能保证在资源空闲时，锁可以被成功地占用，避免了传统锁实现中的繁琐步骤。

### 3.2 更高效的锁实现

- 通过先对锁进行简单检查，然后再使用 CAS 进行获取，可以减少对总线的负载：

  ```
  cpp复制代码void lock(Lock* l) {
      while (1) {
          while (*l == 1);  // 自旋等待锁空闲
          if (atomicCAS(l, 0, 1) == 0)  // 尝试获取锁
              return;
      }
  }
  ```

  - **优点**：在锁空闲时可以立即获取，减少了繁琐的竞争。

------

## 4. 数据结构示例：有序链表

### 4.1 问题描述

- 多个线程在一个有序链表上进行插入或删除操作可能导致数据丢失或链表损坏。
- 例如：
  - 线程 1 插入值 `6`，线程 2 插入值 `7`，由于这两个线程对链表的修改不加以控制，可能导致插入的一个值被丢失。

### 4.2 单全局锁的解决方案

- 对整个链表使用一个全局锁，保证在任意时刻只有一个线程能够对链表进行修改。
  - **优点**：实现简单。
  - **缺点**：**并发性差**，无法同时对链表的不同部分进行操作，导致整体性能下降。

### 4.3 手递手锁（Hand-Over-Hand Locking）

- **手递手锁**是一种细粒度的锁定机制，每个节点都有自己的锁，遍历链表时获取当前节点的锁并释放前一个节点的锁，以实现更高的并行度。
- **优点**：允许对链表不同部分的并发访问，增加了系统的吞吐量。
- **缺点**：复杂性增加，需要在遍历的每一步都进行锁操作。

------

## 5. 无锁数据结构

### 5.1 无锁算法的定义

- **无锁算法**保证即使某个线程被暂停或崩溃，系统中仍有其他线程可以继续前进。
- 使用原子操作如 `compare_and_swap` 来实现数据结构操作的原子性。

### 5.2 无锁栈实现（第一次尝试）

- 基本思路：使用 

  ```
  compare_and_swap
  ```

   尝试修改栈顶指针，确保只有在栈顶未被其他线程修改时才能成功更新。

  ```
  cpp复制代码struct Node {
      Node* next;
      int value;
  };
  
  struct Stack {
      Node* top;
  };
  
  void push(Stack* s, Node* n) {
      while (1) {
          Node* old_top = s->top;
          n->next = old_top;
          if (compare_and_swap(&s->top, old_top, n) == old_top)
              return;
      }
  }
  
  Node* pop(Stack* s) {
      while (1) {
          Node* old_top = s->top;
          if (old_top == NULL)
              return NULL;
          Node* new_top = old_top->next;
          if (compare_and_swap(&s->top, old_top, new_top) == old_top)
              return old_top;
      }
  }
  ```

  - **注意**：这种实现方式存在**ABA 问题**，即栈顶指针在某线程读取后，可能经过若干次变化又回到原值，导致错误判断。

### 5.3 解决 ABA 问题的方法

- **引入计数器**：为栈顶维护一个计数器，每次修改栈顶时增加计数，通过双重比较交换（`double compare_and_swap`）来同时检查指针和计数器，确保数据一致性。
- **Hazard Pointer**：使用**危险指针**（Hazard Pointer）来跟踪每个线程正在访问的节点，避免其他线程释放这些节点。

------

## 6. 锁与无锁数据结构的性能比较

- 在一些应用场景中，传统锁机制的代码性能可能与无锁代码相当甚至更快，尤其是在**线程数较少或竞争不激烈**的情况下。
- 但是在存在大量线程或频繁的上下文切换的情况下，锁机制可能会导致**优先级反转**、**队列效应**等性能问题，无锁数据结构能够更好地适应这种环境。

------

## 7. 总结

- **细粒度锁定**通过将锁的范围缩小来增加并行度，减少了全局锁带来的争用问题，但增加了实现的复杂度和可能的死锁风险。
- **无锁编程**提供了一种避免使用锁进行同步的方法，可以避免死锁，并在一些场景下提高系统性能。
- **实现无锁数据结构的挑战**在于确保正确性和一致性，例如解决 ABA 问题以及避免引用已释放的内存。

------

## 8. 更多阅读

- **Michael and Scott (1996)**: 提供了简单、快速且实用的无锁和有锁并发队列算法。

- **Harris (2001)**: 对无锁链表的实际实现进行了探讨。

- 相关博客和文章

  ：

  - [Lock-Free Code: A False Sense of Security](http://www.drdobbs.com/cpp/lock-free-code-a-false-sense-of-security/210600279)
  - [Common Pitfalls in Writing Lock-Free Algorithms](http://developers.memsql.com/blog/common-pitfalls-in-writing-lock-free-algorithms/)











































# 第18课：事务内存（Transactional Memory）

---

## 1. 课程概述
- 本节课讨论了**事务内存（Transactional Memory, TM）**的概念，及其在并行编程中如何简化同步操作的实现。
- 事务内存允许开发者定义一个代码块为**原子操作（Atomic Operation）**，系统将确保在运行时正确维护原子性和隔离性。

---

## 2. 提高同步抽象层次
### 2.1 背景
- 之前的同步方式：
  - 使用底层的**机器级原子操作**，如 `fetch-and-op`、`test-and-set`、`compare-and-swap`（CAS）、`load linked-store conditional`。
  - 使用这些原子操作构建了高级同步原语，如**锁（Locks）**和**屏障（Barriers）**。
  - 这些方法虽然功能强大，但很容易导致错误，如**原子性违背（Atomicity Violation）**和**死锁（Deadlock）**。

### 2.2 事务内存的引入
- **事务内存**提高了同步的抽象层次，允许开发者简单地声明一个代码块为原子事务：
  ```cpp
  atomic {
      // 保持原子性
      int tmp = bank.get(account);
      tmp += amount;
      bank.put(account, tmp);
  }

- 开发者只需声明需要原子性保障的代码块，系统负责实现同步。

------

## 3. 什么是事务？

- **事务内存（Transactional Memory, TM）**是一种并行编程模型，借鉴了数据库事务的概念，提供以下性质：
  - **原子性（Atomicity）**：事务的操作要么全部执行，要么全部不执行。
  - **隔离性（Isolation）**：在事务提交之前，其写入的结果对其他线程不可见。
  - **可串行化（Serializability）**：事务的执行结果看起来像是按照某一顺序串行执行的。

### 3.1 事务的基本属性

- **原子性**：当事务提交时，所有写入操作一次性生效；若发生回滚，则事务的所有写入均被撤销。
- **隔离性**：事务中的操作在完成之前，对其他线程不可见。
- **可串行化**：多个事务的执行顺序不需要事先确定，但最终结果相当于按某个顺序串行执行。

------

## 4. 事务内存 vs 锁

- 使用事务内存的代码实现与使用锁的代码相比有以下不同：
  - **声明式（Declarative）**：程序员只需声明要维护原子性的代码块，而无需显式管理锁。
  - **系统负责实现同步**：系统可以使用锁实现 `atomic {}`，或者采用其他方式，比如**乐观并发控制（Optimistic Concurrency Control）**，仅在实际发生读写冲突时进行处理。

### 4.1 事务 vs 细粒度锁

- 事务内存结合了细粒度锁的高并发性和粗粒度锁的易编程性：
  - 细粒度锁实现较复杂，容易引入死锁和性能问题。
  - 事务内存可以自动处理并发，程序员只需关注逻辑的正确性。

------

## 5. 数据版本管理策略

- 事务内存需要管理事务中数据的

  新版本（Uncommitted Version）**和**旧版本（Committed Version）：

  1. 急迫版本管理（Eager Versioning）：
     - 内存立即更新，使用**撤销日志（Undo Log）**在事务失败时恢复数据。
     - **优点**：提交速度快，因为数据已在内存中。
     - **缺点**：事务回滚较慢，可能存在容错问题。
  2. 懒惰版本管理（Lazy Versioning）：
     - 写入操作先缓存在**写缓冲区（Write Buffer）**中，提交时再更新到内存。
     - **优点**：回滚速度快，因为只需清空写缓冲区。
     - **缺点**：提交速度较慢。

------

## 6. 冲突检测策略

- 冲突检测是事务内存的核心功能之一，用于确定两个并发事务之间是否发生了冲突。
  1. 悲观检测（Pessimistic Detection）：
     - 在每次内存访问时立即检查是否存在冲突。
     - **优点**：可以早期检测冲突，减少无用的计算。
     - **缺点**：可能导致更高的检测开销，缺乏前向进展的保证。
  2. 乐观检测（Optimistic Detection）：
     - 仅在事务尝试提交时检测冲突。
     - **优点**：减少冲突检测的开销，能够更好地保证进展。
     - **缺点**：在提交阶段可能会发现冲突并导致回滚。

------

## 7. 冲突检测的粒度

- 冲突检测的粒度影响事务内存的性能和精度：
  - **对象粒度（Object Granularity）**：降低时间和空间开销，但可能会引入虚假共享。
  - **机器字粒度（Machine Word Granularity）**：减少虚假共享，但增加了时间和空间开销。
  - **缓存行粒度（Cache Line Granularity）**：在对象和字粒度之间折中。

------

## 8. 事务内存的实现类型

### 8.1 软件事务内存（STM）

- 通过纯软件实现事务内存机制。
- **优点**：不需要硬件支持，可以灵活实现不同策略。
- **缺点**：性能受到软件开销限制。

### 8.2 硬件事务内存（HTM）

- 使用硬件支持实现事务管理。
- 实现方式：
  - **数据版本管理**通过缓存实现，例如将写缓冲区或撤销日志保存在缓存中。
  - **冲突检测**通过**缓存一致性协议（Cache Coherence Protocol）**实现，例如使用嗅探协议来检测读写冲突。
  - **处理器要求**：需要在事务开始时对寄存器进行**检查点（Checkpoint）**保存，以便在事务回滚时恢复状态。

### 8.3 混合事务内存（Hybrid TM）

- 结合硬件和软件实现的事务内存，兼具两者的优势，提升性能和灵活性。

------

## 9. 硬件事务内存（HTM）示例

- **Intel Haswell 架构**引入了对事务内存的支持，提供了指令 `xbegin`、`xend` 和 `xabort`，用于开始、结束和中止事务。
- 实现的基本方式是使用 L1 缓存来跟踪读集和写集，并在提交时确保所有内存操作原子地生效。

------

## 10. 事务内存的优势

1. **易用性**：开发者只需声明原子性需求，系统负责实现正确的同步。
2. **高性能**：在许多情况下，事务内存的性能接近于细粒度锁，但开发复杂度大大降低。
3. **失败原子性和恢复**：当线程失败时，系统会自动中止并回滚事务，确保系统状态一致。
4. **可组合性**：事务内存支持安全地组合不同模块，避免了锁机制下可能出现的死锁和复杂的系统范围策略。

------

## 11. 总结

- **事务内存**提供了一种简单而强大的并行编程同步抽象，通过声明式的原子性保证，降低了编写正确并行程序的难度。
- **不同的实现方式**（硬件、软件、混合）具有各自的优缺点，适用于不同的应用场景。
- 事务内存的**数据版本管理**和**冲突检测**是实现其核心功能的关键，需要在性能和正确性之间进行权衡。











































































# 第 19 课：异构计算架构（Heterogeneous Computing Architecture）

---

## 1. 课程概述
- 本节课讨论了**异构计算架构**，即使用多种不同类型的计算单元（如 CPU、GPU 和特定用途加速器）来提高计算性能。
- 重点介绍如何有效地组合和调度这些异构单元，以获得更高的系统吞吐量和能效。

---

## 2. 异构计算架构的定义
- **异构计算（Heterogeneous Computing）**指的是系统由多种类型的计算单元组成，例如 CPU、GPU、FPGA 等。
- 这些计算单元在性能、能耗、适合的工作负载上各有特点。通过将它们组合在一起，可以**针对不同类型的任务**实现更高的效率。

### 2.1 CPU vs. GPU
- **CPU（中央处理器）**擅长执行复杂的控制流任务，具有较高的指令执行速度。
- **GPU（图形处理器）**擅长并行数据处理，特别适用于具有高并行度的任务，如图形渲染和矩阵计算。

### 2.2 专用硬件加速器
- 例如 **Tensor Processing Units（TPU）** 和 **ASIC**，用于特定的任务，能效非常高，但灵活性较低。

---

## 3. 异构系统的设计挑战
### 3.1 任务分配和调度
- **异构任务调度（Task Scheduling）**：根据任务的特性，将其分配给最合适的计算单元。例如，密集计算任务可以分配给 GPU，而逻辑控制任务则适合 CPU。
- **负载均衡**是设计异构系统时面临的一个重要问题。任务的适当分配可以有效地提高资源利用率。

### 3.2 数据移动
- **内存管理和数据移动**在异构计算中非常关键。数据在 CPU 和 GPU 之间的移动是性能瓶颈之一，特别是在大量数据传输时。
- **统一内存（Unified Memory）**：一种解决方案，使不同计算单元可以方便地共享内存空间，减少显式的数据拷贝操作。

---

## 4. 编程异构系统
### 4.1 CUDA 和 OpenCL
- **CUDA** 是 NVIDIA 提供的 GPU 编程框架，适合高性能并行计算，允许开发者直接访问 GPU 的内存和线程管理。
- **OpenCL** 是一个开放标准，支持多种硬件的并行编程，包括 CPU、GPU 和 FPGA。它提供了一个通用的编程接口，使得代码可以跨平台运行。

### 4.2 高层抽象
- **领域专用框架**（如 TensorFlow、PyTorch 等）提供了较高的抽象层，使得开发者可以在不直接处理底层硬件的情况下使用异构计算资源。
- 这些框架会自动选择和分配任务到最合适的计算单元，帮助开发者高效利用硬件资源。

---

## 5. 案例分析：CPU + GPU 协同计算
- 在一些典型应用中，例如**深度学习**和**科学模拟**，可以通过**CPU + GPU 协同计算**来加速训练过程：
  - **数据预处理**和**任务调度**可以在 CPU 上完成，**矩阵计算**和**卷积操作**则分配给 GPU。
  - 这样可以有效地将复杂度较低但数据密集的任务与需要更高计算能力的任务分开处理，从而提高整体效率。

---

## 6. 小结
- **异构计算架构**通过结合多种不同的计算单元，实现了计算效率和能效的显著提升。
- 在设计异构系统时，关键的挑战包括如何**有效地调度任务**，如何**管理内存和数据移动**，以减少瓶颈并提高利用率。
- 对于开发者而言，了解 CUDA、OpenCL 等工具，并使用高层框架（如 TensorFlow）进行异构编程，是充分利用现代硬件的关键。

---

这份笔记总结了异构计算架构的定义、设计挑战及其应用，帮助理解如何利用不同计算单元的优势，通过高效的调度和内存管理来提升系统的性能和能效。# 第 19 课：异构计算架构（Heterogeneous Computing Architecture）

---

## 1. 课程概述
- 本节课讨论了**异构计算架构**，即使用多种不同类型的计算单元（如 CPU、GPU 和特定用途加速器）来提高计算性能。
- 重点介绍如何有效地组合和调度这些异构单元，以获得更高的系统吞吐量和能效。

---

## 2. 异构计算架构的定义
- **异构计算（Heterogeneous Computing）**指的是系统由多种类型的计算单元组成，例如 CPU、GPU、FPGA 等。
- 这些计算单元在性能、能耗、适合的工作负载上各有特点。通过将它们组合在一起，可以**针对不同类型的任务**实现更高的效率。

### 2.1 CPU vs. GPU
- **CPU（中央处理器）**擅长执行复杂的控制流任务，具有较高的指令执行速度。
- **GPU（图形处理器）**擅长并行数据处理，特别适用于具有高并行度的任务，如图形渲染和矩阵计算。

### 2.2 专用硬件加速器
- 例如 **Tensor Processing Units（TPU）** 和 **ASIC**，用于特定的任务，能效非常高，但灵活性较低。

---

## 3. 异构系统的设计挑战
### 3.1 任务分配和调度
- **异构任务调度（Task Scheduling）**：根据任务的特性，将其分配给最合适的计算单元。例如，密集计算任务可以分配给 GPU，而逻辑控制任务则适合 CPU。
- **负载均衡**是设计异构系统时面临的一个重要问题。任务的适当分配可以有效地提高资源利用率。

### 3.2 数据移动
- **内存管理和数据移动**在异构计算中非常关键。数据在 CPU 和 GPU 之间的移动是性能瓶颈之一，特别是在大量数据传输时。
- **统一内存（Unified Memory）**：一种解决方案，使不同计算单元可以方便地共享内存空间，减少显式的数据拷贝操作。

---

## 4. 编程异构系统
### 4.1 CUDA 和 OpenCL
- **CUDA** 是 NVIDIA 提供的 GPU 编程框架，适合高性能并行计算，允许开发者直接访问 GPU 的内存和线程管理。
- **OpenCL** 是一个开放标准，支持多种硬件的并行编程，包括 CPU、GPU 和 FPGA。它提供了一个通用的编程接口，使得代码可以跨平台运行。

### 4.2 高层抽象
- **领域专用框架**（如 TensorFlow、PyTorch 等）提供了较高的抽象层，使得开发者可以在不直接处理底层硬件的情况下使用异构计算资源。
- 这些框架会自动选择和分配任务到最合适的计算单元，帮助开发者高效利用硬件资源。

---

## 5. 案例分析：CPU + GPU 协同计算
- 在一些典型应用中，例如**深度学习**和**科学模拟**，可以通过**CPU + GPU 协同计算**来加速训练过程：
  - **数据预处理**和**任务调度**可以在 CPU 上完成，**矩阵计算**和**卷积操作**则分配给 GPU。
  - 这样可以有效地将复杂度较低但数据密集的任务与需要更高计算能力的任务分开处理，从而提高整体效率。

---

## 6. 小结
- **异构计算架构**通过结合多种不同的计算单元，实现了计算效率和能效的显著提升。
- 在设计异构系统时，关键的挑战包括如何**有效地调度任务**，如何**管理内存和数据移动**，以减少瓶颈并提高利用率。
- 对于开发者而言，了解 CUDA、OpenCL 等工具，并使用高层框架（如 TensorFlow）进行异构编程，是充分利用现代硬件的关键。



























































# 第 20 课：领域专用语言的设计与实现（Domain-Specific Language Design and Implementation）

---

## 1. 课程概述
- 本节课探讨了**领域专用语言（Domain-Specific Languages, DSLs）**的概念、重要性以及如何有效地设计和实现它们。
- DSL 是针对特定应用领域而设计的编程语言，旨在提高开发者的工作效率和代码的可维护性。

---

## 2. DSL 的定义和作用
- **DSL** 是为某一特定领域或问题空间提供高效表达能力的编程语言，与通用编程语言相比，它的目标更专一。
- 例如，SQL 用于数据库查询，正则表达式用于文本匹配。
- 通过专用语言，可以减少程序员为实现特定任务所需的代码量，并提升可读性和领域抽象层次。

### 2.1 DSL 的优点
- **提高生产力**：通过简洁的语法和专用的操作，提高特定领域任务的开发效率。
- **领域抽象**：减少与底层实现的耦合，让开发者可以集中于领域特定的逻辑和需求。

### 2.2 DSL 的类型
- **内部 DSL（Internal DSL）**：嵌入在现有编程语言中的领域语言，如 Scala 中的 Spark API。
- **外部 DSL（External DSL）**：完全独立的语言，通常需要独立的解析器和编译器。

---

## 3. DSL 的设计原则
### 3.1 语法设计
- **DSL 语法应简单明了**，尽量减少歧义性，确保领域专家也能快速掌握。
- **采用领域术语**，避免计算机科学中的过于通用的概念，使得语言更加自然贴合领域使用者的思维。

### 3.2 语义模型
- 设计 DSL 时，要明确其背后的**语义模型**，即每一个语言元素具体要执行什么样的计算或操作。
- 例如，图处理领域的 DSL 应该包含像“节点”、“边”等核心概念，能够自然地表达图上的操作，如遍历或路径查找。

---

## 4. DSL 的实现
### 4.1 解释器 vs 编译器
- **解释器（Interpreter）**：直接将 DSL 代码解释执行，适用于快速开发和原型设计。
- **编译器（Compiler）**：将 DSL 代码转换为目标代码（如 C/C++ 代码），以获得更好的性能。

### 4.2 实现的步骤
1. **词法分析（Lexical Analysis）**：将 DSL 源代码分解为最小的语言单元（Tokens）。
2. **语法分析（Parsing）**：构建**抽象语法树（AST, Abstract Syntax Tree）**，表示代码的逻辑结构。
3. **语义分析（Semantic Analysis）**：检查代码的正确性并进行类型检查等验证。
4. **代码生成（Code Generation）**：将 AST 转换为目标代码，或者直接解释执行。

### 4.3 框架和工具
- 可以利用现有的工具如 **ANTLR**、**LLVM** 来简化 DSL 的实现。ANTLR 可以生成词法分析器和语法分析器，而 LLVM 则可以帮助将 DSL 转换为高效的目标代码。

---

## 5. 案例研究：图计算领域专用语言
### 5.1 领域介绍
- 图计算在社交网络、推荐系统等领域有广泛应用，常见的图计算包括**PageRank**、**最短路径查找**等。
- 为了高效地处理这些计算任务，可以设计专用的 DSL，使得用户可以以更自然的方式描述图操作。

### 5.2 GraphLab 的 DSL 实现
- **GraphLab** 作为一个图计算框架，提供了便捷的 API，允许开发者定义图计算中的顶点和边的操作。
- DSL 在这里的作用是提高描述图计算的能力，例如，用户可以用一行代码实现节点的更新逻辑，而不必关心低层的图遍历机制。

---

## 6. 小结
- **领域专用语言（DSL）**是提高特定领域开发效率的重要工具，通过为开发者提供简洁、专用的抽象，可以减少冗余代码并提升可读性。
- 在 DSL 的实现上，可以选择**解释器**或**编译器**的方式，根据项目的需要来权衡**灵活性**和**性能**。
- 案例如 **GraphLab** 展示了 DSL 在图计算领域的威力，通过提供高效、易用的 API，开发者可以专注于高层的图计算逻辑。

























































# 第 21课：图领域专用语言的编程

---

## 1. 课程概述
- 本节课讨论了如何为**图计算（Graph Computation）**设计专用编程系统，以应对大规模图数据的复杂处理。
- 图计算被广泛用于现代应用中，如**网页搜索**、**推荐系统**、**广告**、**异常检测**等。

---

## 2. 图计算的挑战与设计方向
### 2.1 图计算的广泛应用
- 典型数据集包括社交网络（如 Twitter 社交图）、知识图谱（如 Wikipedia）、电影和用户互动（如 IMDB 和 Netflix）等。

### 2.2 专用编程系统的趋势
- 现代计算机系统通常是并行且异构的，这对程序员提出了极高的挑战。
- **领域专用编程系统**放弃了部分通用性，专注于特定问题空间的高效性，从而提高生产率和性能。
- **性能可移植性（Performance Portability）**是一个重要目标：在不同平台上运行同样的程序仍然能取得良好的性能。

---

## 3. 分析图的编程系统设计
### 3.1 图编程的基本抽象
- 如果我们要设计一个用于图计算的编程系统，首先需要回答以下问题：
  - **需要哪些基本操作**才能方便地表达和有效执行？
  - **最重要的优化策略**有哪些？这些高层次抽象不应该妨碍这些优化，甚至应当让系统能为应用执行它们。

### 3.2 案例分析：PageRank 算法
- **PageRank**是一个典型的迭代图算法：
  - 节点表示网页，边表示页面间的链接。
  - 每个页面的排名取决于链接到它的其他页面的排名。
  - PageRank 使用公式：
                                                 $$ R[i] = \frac{1-\alpha}{N} + \alpha \sum_{j \in \text{links to } i} \frac{R[j]}{\text{Outlinks}[j]} $$
  - 其中，$R[i]$ 表示页面 $i$ 的排名，$\alpha$ 是折扣系数，$N$ 是总页面数。

---

## 4. GraphLab：用于迭代计算的图计算系统
### 4.1 GraphLab 的基本概念
- **GraphLab** 是一个用于描述图上迭代计算的系统，实现为一个 C++ 库，支持共享内存和分布式计算。
- **图的表示**：图 $G = (V, E)$，每个顶点 $v$ 和边 $u \to v$ 都包含数据。
- **全局数据**：图上可以有只读的全局数据，而不是针对每个顶点或边的数据。

### 4.2 GraphLab 的操作：顶点程序
- 每个**顶点程序（vertex program）**定义在该顶点的**局部邻域**上。
- 顶点的邻域包括：
  - 当前顶点
  - 邻接边和邻接顶点
- 例如，在 PageRank 中，每个顶点程序计算其邻居的排名，然后更新自身的排名。

### 4.3 GraphLab 的调度机制
- **信号传递机制**：通过信号来生成新的计算任务。
- 例如，可以通过调用 `vertex.signal()` 来将某个顶点重新加入到任务队列中，以便在未来再次执行。

### 4.4 并行一致性模型
- GraphLab 允许程序员指定不同粒度的原子性来决定并行度：
  - **全局一致性**：确保在顶点程序运行时，其他执行不能访问或修改其范围内的数据。
  - **边一致性**和**顶点一致性**提供较低的并行保障，从而提高系统的并行性。

---

## 5. Ligra：简化并行图操作的框架
### 5.1 基本操作：EDGEMAP 和 VERTEXMAP
- **Ligra** 提供了简单的图计算操作，旨在支持并行图遍历：
  - **EDGEMAP**：遍历顶点集合的所有邻接边，并执行指定的更新函数。
  - **VERTEXMAP**：对顶点集合中的所有顶点执行某个函数。
- 例如，广度优先搜索（BFS）可以通过 EDGEMAP 和 VERTEXMAP 组合实现。

### 5.2 PageRank 在 Ligra 中的实现
- 使用 EDGEMAP 和 VERTEXMAP 来迭代更新每个顶点的排名，直到结果收敛。
- 该过程包括对每个顶点的邻居进行遍历，并累加其排名贡献。

---

## 6. 图计算优化：提高大图操作的性能
### 6.1 增加局部性
- 通过重新组织图结构来提高**数据局部性**，减少对非连续内存的访问，提高缓存效率。

### 6.2 图压缩
- **图压缩**可以减少内存带宽的需求，从而提高整体性能。
- 在图数据上应用压缩技术，在读取数据时进行解压，使用 CPU 来换取更小的内存占用。
- 例如，在存储边列表时，可以使用**增量编码**（例如存储边索引的差值而不是绝对值）。

### 6.3 图分片（Sharding）
- 将图分片，每片包含一个顶点子集及其所有的入边。
- **GraphChi** 使用这种方法进行大规模图计算，以在单台 PC 上处理超大规模图数据。

---

## 7. 小结
- 图计算是现代计算中非常重要的一类任务，涉及复杂的并行和同步挑战。
- 为了解决这些问题，专用图计算框架如 **GraphLab** 和 **Ligra** 提供了抽象和优化，帮助程序员高效地表达图操作。
- 高性能的图计算依赖于：
  1. **并行性**：需要解决更新共享顶点或边的复杂性。
  2. **局部性优化**：通过结构化图以提高 I/O 效率。
  3. **图压缩**：减少内存带宽或磁盘 I/O 的需求。





















































# 第 22 课：Spark 和大规模并行计算

---

## 1. 课程概述
- 本节课介绍了 **Apache Spark**，一个用于处理大规模数据集的分布式计算框架。
- Spark 提供了**内存中的性能**和**跨集群的容错性**，使得处理大规模数据变得更为高效。

---

## 2. Spark 编程模型
### 2.1 以块为粒度的程序结构
- Spark 允许开发者以更高的抽象级别定义计算，避免处理底层的线程和锁。
- 这种块粒度的结构使得数据在处理过程中能保持更高的局部性，减少 I/O 和网络通信开销。

### 2.2 RDD（弹性分布式数据集）
- **RDD（Resilient Distributed Dataset）** 是 Spark 的核心抽象：
  - **只读的有序记录集合**，存储在内存中。
  - 可以通过在存储系统中对数据进行确定性转换（如 map、filter 等）来创建 RDD。
  - 对 RDD 的操作可分为**转换（Transformations）**和**行动（Actions）**。

### 2.3 RDD 的操作
- **转换（Transformations）**：生成新的 RDD，例如 `map()`、`filter()`、`reduceByKey()`。
- **行动（Actions）**：将数据返回给应用，例如 `count()`、`collect()`。

示例：
```scala
// 创建 RDD
var lines = spark.textFile("hdfs://15418log.txt")
// 过滤出移动客户端相关的行
var mobileViews = lines.filter((x: String) => isMobileClient(x))
// 计算 RDD 中元素的数量
var numViews = mobileViews.count()
```

## 3. RDD 的实现与存储

- **RDD 的存储**：RDD 可以存储在内存中，以减少重复计算的开销。
- RDD 分区（Partitioning）：
  - RDD 通常被分为多个**分区（Partitions）**，每个分区是一个数据块，可以独立处理。
  - 通过分区实现数据的并行处理，提高系统的整体吞吐量。

### 3.1 窄依赖与宽依赖

- **窄依赖（Narrow Dependencies）**：每个父 RDD 的分区最多被一个子 RDD 的分区引用。允许操作的**融合（Fusion）**，减少了内存开销和通信开销。
- **宽依赖（Wide Dependencies）**：一个父 RDD 的分区可能被多个子 RDD 的分区引用，通常需要**全量的分区数据交换**，增加了通信负担。

------

## 4. 容错性和任务调度

### 4.1 通过谱系图实现容错

- **谱系（Lineage）**：记录 RDD 的转换操作序列。通过谱系图，Spark 能在节点故障时重新计算丢失的数据分区。
- 例如，若某节点失效，可以通过重新执行该节点之前的转换操作来重建对应的 RDD。

### 4.2 调度任务以提高效率

- Spark 的任务调度器有以下职责：
  - **数据局部性**：尽量将计算安排在数据所在的节点上，减少数据移动的开销。
  - **处理节点故障**：在任务失败时，调度器会将任务重新分配到其他节点进行重试。
  - **处理慢速节点**：在节点运行缓慢时，调度器可以在其他节点上**重复执行（Speculative Execution）**，以减少整体延迟。

------

## 5. 使用 Spark 进行迭代计算

- Spark 通过将中间数据保存在内存中，显著加速了迭代式计算，例如机器学习和图计算。
- PageRank 示例：
  - PageRank 算法可以使用 Spark 实现为一个迭代过程，重复更新每个网页的排名。
  - 在每次迭代中，计算每个网页的排名贡献，并更新整个图的排名。

```
scala复制代码// 加载图，创建初始的 RDD
val links = spark.textFile("hdfs://links.txt").map(...).persist()
var ranks = // 初始的 (URL, rank) 对

for (i <- 1 to NUM_ITERATIONS) {
  // 计算每个网页的贡献
  val contribs = links.join(ranks).flatMap {
    case (url, (links, rank)) => links.map(dest => (dest, rank / links.size))
  }
  // 计算新的排名
  ranks = contribs.reduceByKey(_ + _).mapValues(sum => a / N + (1 - a) * sum)
}
```

------

## 6. Spark 与传统 MapReduce 的比较

### 6.1 MapReduce 的局限性

- MapReduce 强制使用 `map` 和 `reduce` 结构，程序结构简单，但缺乏灵活性。
- **迭代计算的低效**：每次迭代都需要从磁盘中重新加载数据，导致大量的 I/O 开销。

### 6.2 Spark 的优势

- **内存中的迭代计算**：Spark 在内存中保持中间数据，避免频繁的磁盘 I/O，使得迭代计算更快。
- **丰富的编程接口**：Spark 提供了更丰富的 API，如 `filter()`、`join()`、`groupByKey()` 等，适合更复杂的数据处理任务。
- **支持交互式查询**：Spark 可以在内存中对大规模数据集进行交互式查询，延迟仅为几秒钟。

------

## 7. Spark 中的优化技术

### 7.1 缓存和持久化

- 可以使用 `.persist()` 或 `.cache()` 方法将 RDD 缓存在内存中，从而加速后续对该数据的操作。
- 持久化级别：
  - **内存持久化（Memory Only）**：只保留在内存中。
  - **磁盘持久化（Memory and Disk）**：当内存不足时，将部分数据写入磁盘。

### 7.2 分区和数据分布

- 使用 `.partitionBy()` 可以手动指定 RDD 的分区方式，通常采用**哈希分区（HashPartitioner）\**或\**范围分区（RangePartitioner）**，以减少宽依赖带来的数据传输开销。
- 在进行诸如 `join()` 的操作时，如果两个 RDD 使用相同的分区函数，可以大大减少数据的传输。

------

## 8. 小结

- **Spark** 是一个高效的分布式计算框架，特别适用于大规模数据集的处理。
- RDD 是 Spark 的核心抽象，它允许开发者在保证容错的同时，以一种简单的方式表达复杂的数据操作。
- 通过内存中的迭代计算和任务的高效调度，Spark 提供了比传统 MapReduce 更好的性能，尤其在处理迭代式任务和交互式查询时。













































# 第 23 课：高效评估深度神经网络（Efficiently Evaluating Deep Neural Networks）

---

## 1. 课程概述
- 本节课探讨了如何在硬件和算法层面高效评估深度神经网络（Deep Neural Networks, DNN），并强调了并行计算对提升模型推理速度的重要性。
- 内容包括对卷积神经网络（Convolutional Neural Networks, CNN）优化方法的深入探讨，以及基于硬件架构的加速技术。

---

## 2. 深度学习的主要计算瓶颈
- 深度神经网络的计算瓶颈主要体现在两个方面：
  - **计算量巨大**：特别是 CNN 中的卷积操作，涉及大量的矩阵乘法。
  - **数据移动的开销高**：需要在多个内存层级之间频繁地传输数据。

### 2.1 卷积层的计算需求
- 卷积神经网络的卷积层是计算最密集的部分，需要对输入特征图和滤波器进行大量的点积操作。
- 针对每层卷积，计算量可以表示为：
  \[
  O(K^2 \times C_{in} \times C_{out} \times H \times W)
  \]
  其中，$K$ 为卷积核大小，$C_{in}$ 和 $C_{out}$ 分别为输入和输出通道数，$H$ 和 $W$ 为输入特征图的高度和宽度。

---

## 3. 提高深度网络评估效率的方法
### 3.1 矩阵分块（Blocking）
- **矩阵分块（Blocking）**是为了提升矩阵乘法的缓存利用率。通过将矩阵分成更小的块，可以更好地利用 L1 和 L2 缓存，减少数据从内存加载的次数。
- 矩阵分块可以大大减少由于缓存不命中而导致的延迟，从而提高整体计算效率。

### 3.2 Winograd 算法
- **Winograd 卷积算法**通过数学重构减少了卷积运算所需的乘法次数，从而提高了计算速度。
- 适用于小卷积核（如 $3 \times 3$）的卷积层，特别是在内存带宽受限的情况下具有明显的加速效果。

### 3.3 FFT 加速卷积
- 使用**快速傅里叶变换（FFT）**来实现卷积运算，将空间域的卷积转换为频域中的乘法运算，可以减少计算复杂度。
- 对于较大的卷积核，FFT 变换能带来显著的加速。

---

## 4. 并行优化策略
### 4.1 数据并行
- 在进行推理时，可以将输入数据拆分到多个处理器（如 GPU 核心）上进行并行计算。
- **数据并行**的关键在于各处理器之间的数据同步和负载均衡。通过让多个处理单元同时计算不同批次的数据，可以显著提高吞吐量。

### 4.2 模型并行
- **模型并行**在参数较多的模型中尤其重要。当单个 GPU 无法存储整个模型时，可以将模型分割成多个部分，分配到不同的 GPU 上。
- 各部分模型需要在推理过程中相互传递中间结果，因此通信开销和计算的同步是模型并行的主要挑战。

### 4.3 内存优化
- 由于 DNN 的计算涉及大量的内存访问，通过**重用中间结果**以及**减少内存分配次数**，可以有效减少内存带宽的需求，从而提升性能。
- 在 GPU 上，可以通过**流式多处理器（SM）**的高效管理来优化内存访问，确保尽量少的等待时间。

---

## 5. 基于硬件的优化
### 5.1 专用硬件加速器
- **TPU（Tensor Processing Unit）**：Google 开发的专用硬件，用于加速深度学习模型的推理。TPU 采用低精度计算，并结合了高效的内存架构设计，以减少计算和内存之间的瓶颈。
- **FPGA 和 ASIC**：定制硬件可以实现针对特定模型的高效运算，通常比通用处理器具有更高的能效比。

### 5.2 减少数据移动
- **Near Memory Computing（近存计算）**：通过将计算单元与存储单元紧密耦合，减少数据在计算与存储之间的长距离移动，降低功耗和延迟。
- **内存内计算（In-Memory Computing）**：将部分计算逻辑集成到内存中，直接在存储器内部进行简单的计算操作，从而进一步减少数据移动。

---

## 6. 案例：AlexNet 和 VGG 的加速
- 对于**AlexNet** 和 **VGG** 等经典卷积神经网络模型，可以使用上述优化方法，如矩阵分块、FFT 和数据并行，来显著提高推理效率。
- **AlexNet** 的卷积层计算量很大，采用分布式数据并行可以有效地提高其推理速度。
- **VGG** 网络由于层数较深，在推理时的主要瓶颈是中间层的内存访问，优化内存访问模式对提高其性能非常重要。

---

## 7. 小结
- **深度神经网络的高效评估**是现代 AI 系统中一个至关重要的任务，通过硬件和算法的优化可以大幅度提升性能。
- **Winograd**、**FFT** 和**近存计算**等技术为深度学习的推理效率提供了有效的提升手段。
- 在实际应用中，应综合利用硬件和软件的优化策略，尤其是在内存访问、数据移动和并行计算方面进行深度优化，以实现最大化的性能提升。























































# 第 24 课：深度神经网络的并行训练

---

## 1. 课程概述
- 本节课探讨了如何加速深度神经网络（Deep Neural Networks, DNN）训练，通过**并行计算**和**数据分片**的方法，使训练过程能够高效地应对大规模数据集和复杂的网络模型。
- 重点内容包括数据并行、模型并行，以及异步更新的使用。

---

## 2. 深度学习的训练目标
- **目标**：学习神经网络的参数，使其能够对任意输入正确分类。
- **训练目标**：最小化**损失函数（Loss Function）**，即希望网络输出的分类结果与真实标签之间的差异尽可能小。
                                                                       $$L = \sum_i L_i$$
  这里 $L_i$ 是每个训练样本 $x_i$ 的损失。

---

## 3. 梯度下降法
### 3.1 梯度下降基本概念
- **梯度下降法（Gradient Descent）**用于调整网络参数，使损失函数下降。
- 对于每个参数，我们需要计算损失函数对该参数的偏导数，然后沿着负梯度方向调整参数，以减少损失。
  ```pseudo
  while (loss too high):
      for each item xi in training set:
          grad += evaluate_loss_gradient(f, params, loss_func, xi)
      params += -grad * step_size

### 3.2 小批量随机梯度下降（Mini-Batch SGD）

- 使用一个随机的小批量训练样本集来计算梯度，而不是使用整个训练集，以减少计算开销。
- 优势在于**计算快**且能获得一个相对较好的梯度估计。

------

## 4. 反向传播（Backpropagation）

### 4.1 基本原理

- **反向传播**用于计算损失函数相对于所有网络参数的梯度。
- 通过链式法则（Chain Rule），计算每一层的梯度，并将它们反向传播至前面的层。
- 例如对于一个单元：    $$ f(x,y,z)=(x+y) z $$
- 使用链式法则，求出每个输入变量的偏导数，以便更新权重。

### 4.2 反向传播的矩阵形式

- 在实际实现中，通过将梯度计算表示为矩阵运算，以更高效地在硬件上执行。
- 对于每一层的权重 $W$，其梯度为：$$ ∂L∂W=XT∂L∂y\frac{\partial L}{\partial W} = X^T \frac{\partial L}{\partial y}∂W∂L=XT∂y∂L$$
-  其中 $X$ 是输入，$\frac{\partial L}{\partial y}$ 是损失对输出的梯度。

------

## 5. 深度神经网络的并行化训练

### 5.1 数据并行训练

- 数据并行（Data Parallelism）

  ：将一个小批量的训练样本分配到多个节点或机器上，每个节点独立计算梯度，最后在一个中心节点进行梯度汇总。

  ```
  pseudo复制代码partition mini-batch across nodes
  for each item xi in mini-batch assigned to local node:
      grad += evaluate_loss_gradient(f, loss_func, params, xi)
  barrier()
  sum reduce gradients, communicate results to all nodes
  barrier()
  update copy of parameter values
  ```

### 5.2 模型并行训练

- **模型并行（Model Parallelism）**：将神经网络的参数分布在不同节点，每个节点负责计算模型的一部分。
- 适用于网络规模太大，以至于单个节点的内存无法存储所有参数的情况。

### 5.3 参数服务器（Parameter Server）设计

- **参数服务器（Parameter Server）**用于管理和更新全局参数。
- 工作节点计算梯度，并将这些梯度发送给参数服务器，由参数服务器更新参数。
  - **异步更新（Asynchronous Update）**：为了避免每次全局同步的高开销，可以采用异步更新方式，即每个节点独立计算并立即发送更新，参数服务器逐步更新参数。

------

## 6. 并行化训练的挑战

### 6.1 节点间的通信瓶颈

- **通信延迟**是并行化训练的一大挑战，尤其在大规模集群上，节点之间的通信开销可能显著降低训练效率。
- 解决方法：
  - **结合树结构的梯度汇总**：代替单一的参数服务器，将梯度汇总的任务分摊到多个节点，以减轻中心节点的压力。

### 6.2 异构节点的性能不一致

- 在集群中，节点的性能可能由于硬件或负载差异而有所不同，导致一些节点较慢而影响整个系统的同步。
- 现代解决方案是利用**异步执行**，各节点不必严格同步，只需逐步更新参数即可。

### 6.3 参数服务器的压力与分片

- **参数服务器的瓶颈**：当多个节点同时向参数服务器请求时，可能导致服务器压力过大。
- 解决方案：
  - **参数分片（Parameter Sharding）**：将参数分布到多个参数服务器上，每个服务器只负责部分参数，减轻单一服务器的压力。

------

## 7. 高性能计算对深度学习训练的影响

### 7.1 使用超级计算机进行模型训练

- 在**超级计算机**上进行模型并行训练，能够有效减少因通信延迟而导致的开销。
- 例如使用高性能互连（如**NVLink**）连接的多 GPU 训练节点，可以实现高带宽、低延迟的数据传输，使得同步代价大幅降低。

### 7.2 使用 FireCaffe 加速训练

- **FireCaffe** 是一种用于大规模 DNN 训练的系统，采用优化的并行化方法，如**结合树结构**和**增大批次**以减少通信频率。
- 通过使用超级计算机（如 Titan 超级计算机）进行训练，FireCaffe 在128个 GPU 上实现了单 GPU 训练的 47 倍加速。

------

## 8. 总结

- 训练深度神经网络需要大量的计算资源，尤其在大规模数据集上进行训练时，涉及到的**计算量和存储需求**极其庞大。
- **数据并行**和**模型并行**是最常用的并行化训练方法，针对不同的场景和硬件环境。
- 现代训练系统普遍采用**参数服务器架构**，结合**异步更新**以应对集群中的通信瓶颈和负载不均衡问题。
- **高性能硬件和优化的并行策略**能够显著提升训练速度，但也需要不断权衡收敛速度和计算复杂性之间的关系。













































# 第 25 课：突破内存墙（Addressing the Memory Wall）

---

## 1. 课程概述
- **内存墙问题**指的是随着处理器速度的提升，内存访问速度跟不上，从而成为系统性能的主要瓶颈。
- 数据的移动不仅会限制性能，还会带来**高能耗**，因此减少数据移动是提高系统性能和能效的关键。

---

## 2. 数据移动的代价
- 数据移动具有很高的能量成本：
  - 执行一个 32 位浮点运算约消耗 $0.9$ pJ 的能量。
  - 从片上**SRAM（本地存储器）**读取数据的能耗为 $5$ pJ。
  - 从低功耗 DDR 内存（LPDDR）加载 32 位数据的能耗为 $640$ pJ。

$$ \text{数据移动的能耗} \gg \text{计算的能耗} $$

---

## 3. 利用局部性减少数据移动
- **局部性**：将经常访问的数据存储在靠近处理器的缓存中，以避免频繁的数据传输。
- **缓存优化**：通过调整计算顺序，例如**循环重排（Loop Blocking）**和**循环融合（Loop Fusion）**，最大化数据的缓存利用。

---

## 4. 循环重构的案例
### 4.1 循环融合（Loop Fusion）
- 通过将多次独立的循环计算合并为一个循环，可以提高**计算强度（Arithmetic Intensity）**，减少对内存的访问。

#### 示例代码
```cpp
void add(int n, float* A, float* B, float* C) {
    for (int i = 0; i < n; i++)
        C[i] = A[i] + B[i];
}

void mul(int n, float* A, float* B, float* C) {
    for (int i = 0; i < n; i++)
        C[i] = A[i] * B[i];
}

// 原始代码中先执行加法，再执行乘法
add(n, A, B, tmp1);
mul(n, tmp1, C, tmp2);
```

通过循环融合，可以将上述操作合并为：

```
cpp复制代码void fused(int n, float* A, float* B, float* C, float* D, float* E) {
    for (int i = 0; i < n; i++)
        E[i] = D[i] + (A[i] + B[i]) * C[i];
}
```

- **算术强度提高**：融合后的代码减少了数据的加载和存储次数，提高了算术强度。

------

## 5. 内存系统架构

### 5.1 DRAM 的基本操作

- DRAM

   的操作包括以下几个阶段：

  1. **预充电（Precharge）**：准备位线，写回行缓冲。
  2. **行激活（Row Activation）**：激活需要访问的行，将其加载到行缓冲区。
  3. **列选择（Column Selection）**：选择特定列，读取数据到总线。

- 这些操作的总延迟会影响 DRAM 的响应速度，

  不同状态下的访问延迟不同

  ，例如：

  - **最优情况**：读取已激活行的数据，只需进行列访问。
  - **最差情况**：需要进行预充电、行激活和列访问。

### 5.2 DRAM 的提升手段

- **突发模式（Burst Mode）**：通过一次命令传输多个数据，减少命令开销，提升数据传输效率。
- **多银行（Multi-Bank）并行**：DRAM 芯片被分为多个独立的**银行（Bank）**，可以并行处理不同的内存请求，从而提高数据引脚的利用率。

### 5.3 内存控制器的角色

- 内存控制器

  负责管理内存请求的调度，包括：

  - **优先调度当前已打开的行**以减少行激活时间。
  - **合并多个小请求**以利用 DRAM 的突发模式传输。
  - **调度策略**：例如**FR-FCFS（First-Ready, First-Come-First-Serve）**，优先处理当前准备好的请求，以最大化行局部性。

------

## 6. 现代内存技术的改进

### 6.1 嵌入式 DRAM（eDRAM）

- **嵌入式 DRAM** 是在 CPU 包中嵌入的一种高速内存，可以用作最后一级缓存（LLC）或者一个单独的地址空间。
- eDRAM 提供的带宽高于传统 DRAM，延迟类似于 DDR4。

### 6.2 堆叠内存（3D Stacked Memory）

- **3D 堆叠内存**通过**硅通孔（Through-Silicon Vias, TSVs）**实现高并行的芯片间连接，使得内存控制器能够直接与多个内存层进行交互，提升带宽并降低延迟。
- 例如，**高带宽内存（High Bandwidth Memory, HBM）**采用 1024 位接口，大幅度提高每个堆栈的带宽。

### 6.3 带宽和功耗优化

- HBM 与 GDDR5 相比，提供更高的带宽功率比，使得每瓦特消耗带来的带宽显著提高，这对图形计算和并行计算任务尤为重要。

------

## 7. 减少数据移动的新策略

### 7.1 将计算移动到数据

- 在某些情况下，**将计算移到数据位置**比将数据移到处理器更为高效。
- 例如，在数据库服务器中，直接在服务器端进行 SQL 查询，而不是将整个数据库传送到客户端来计算。

### 7.2 DRAM 中的硬件加速

- **行复制（Row Copy）**：通过硬件支持的行复制操作，直接在 DRAM 中进行数据复制，减少通过处理器进行 `memcpy` 操作的必要。

------

## 8. 缓存压缩

### 8.1 增加缓存的有效容量

- **缓存压缩**通过压缩缓存中的数据来增加缓存的有效容量。
- 例如，使用**BΔI 压缩**可以通过基准值加偏移的方式存储缓存行中的数据，从而将其压缩成更小的格式。

​                                                                               $$ 值=B+Δ$$

- 缓存压缩可以减少缓存未命中导致的主存访问，提高系统的整体性能。

### 8.2 帧缓冲压缩

- 在 GPU 中，帧缓冲内容可以在传输到内存之前进行无损压缩，例如使用**锚点编码（Anchor Encoding）**来预测并压缩像素块。

------

## 9. 小结

- 内存墙问题

   是现代计算中面临的主要瓶颈，解决这一问题的方法包括：

  1. **提高局部性**：通过软件优化，减少对内存的频繁访问。
  2. **硬件改进**：包括更智能的 DRAM 请求调度，利用嵌入式内存和 3D 堆叠内存等新技术来增加带宽。
  3. **移动计算到数据**：减少不必要的数据传输，通过硬件加速或缓存压缩来优化性能和能耗。













































# 第 26 课：高性能计算的未来（The Future of High-Performance Computing）

---

## 1. 课程概述
- 本节课讨论了当前高性能计算（High-Performance Computing, HPC）系统的现状、挑战和未来的趋势。
- 主要对比了传统超级计算机和数据中心集群在计算密集型和数据密集型应用中的不同特性。

---

## 2. 大规模计算系统对比
- **Oakridge Titan**：第三快的超级计算机，专为**计算密集型应用**设计，如科学模拟。
- **Google 数据中心**：包含大量服务器，用于支持上百万用户，专为**数据收集、存储和分析**设计。

### 2.1 计算类型和特点
- **Titan 超级计算机**：
  - 适合运行**建模与模拟驱动的科学和工程**任务。
  - 强调高性能的计算处理，设计上追求高计算强度。
- **Google 数据中心**：
  - 适用于**网络规模的计算**，如 Web 搜索、语言翻译、视频流传输等。
  - 强调高数据密集度和弹性计算服务的提供。

---

## 3. 超级计算机架构
### 3.1 Titan 的硬件架构
- 每个节点由以下部分组成：
  - **AMD 16 核处理器**
  - **NVIDIA GPU（Kepler 架构）**，每个 GPU 具有 2688 个流处理器，采用单指令多数据（SIMD）并行模型。
  - **38 GB DRAM**，不包括本地磁盘。

### 3.2 Titan 的编程模型
- **Bulk Synchronous Parallel（BSP）模型**：
  - 通过将计算区域划分为多个部分，每个部分由独立的处理器节点进行计算。
  - 采用**同步步骤**来实现所有节点间的数据共享，保证一致性。
- **编程环境**：
  - 节点间通过 **MPI（Message Passing Interface）** 通信。
  - 在节点内部，使用 **OpenMP** 进行 CPU 多线程管理，使用 **CUDA** 进行 GPU 加速。

---

## 4. 数据中心集群和 MapReduce
### 4.1 数据密集型计算
- **MapReduce** 是一个广泛应用于数据密集型应用的编程模型，用于处理大规模的数据集合。
  - **映射（Map）**：将输入数据划分为多个子任务并分配给不同节点处理。
  - **归约（Reduce）**：对来自不同节点的结果进行合并，得到最终结果。
- **优势与挑战**：
  - **优势**：灵活调度、易于处理不同节点负载。
  - **挑战**：高额的存储开销和较低的原始性能。

### 4.2 数据完整性和故障恢复
- 采用数据冗余机制，通常每个文件存储三个副本。
- 在节点故障时，可以通过重新计算丢失的数据并动态重新分配任务来恢复系统状态。

---

## 5. 当前编程系统趋势
- **Spark**：由 UC Berkeley 开发的框架，用于高效地处理数据密集型任务，逐渐发展为一个开源社区的核心工具。
- **GraphLab**：由 CMU 开发的项目，主要用于机器学习算法的描述，支持稀疏矩阵的操作。

---

## 6. 计算趋势的融合
### 6.1 模拟与真实数据结合
- **模拟的局限性**：单纯的模拟难以准确反映真实系统的复杂性。
- **数据的局限性**：依赖真实数据进行分析时，难以捕捉因果关系和“假设”情景。
- **解决方案**：通过在模拟中引入真实数据，或者在数据分析中引入模拟结果，增强模型的准确性和应用广度。

### 6.2 实时分析的可能性
- **大型模拟中的实时分析**：例如，**Millennium XXL Simulation** 生成了 700 TB 的数据，可以在模拟运行时对数据进行分析，以降低数据存储和传输的需求。

---

## 7. 从超级计算到数据中心集群的挑战
### 7.1 超级计算机的挑战
- **可靠性**：超级计算机通常需要定制硬件和严格的容错机制，硬件故障可能导致长时间延迟。
- **静态调度**：任务调度通常是静态的，难以适应节点性能的变化。

### 7.2 数据中心集群的挑战
- **动态分配**：集群的调度是动态的，可以根据当前负载动态调整任务。
- **高层次编程模型**：使用数据为中心的编程模型，适合互联网应用和大数据处理。

---

## 8. Moore 定律和 Dennard 缩放的挑战
### 8.1 Moore 定律的技术挑战
- **特征尺寸的缩小**：逐渐接近原子尺度，制造变得越来越复杂，**光刻技术**在如此小的尺度上面临巨大挑战。
- **统计变异**：在纳米级别上，晶体管特性差异性增加。

### 8.2 Dennard 缩放的终结
- **Dennard 缩放**理论预示着随着晶体管的缩小，功率密度保持不变。
- 然而，随着缩小到 1V 以下的限制，芯片的功率不再保持恒定，从 2004 年起功率密度成为主要瓶颈，导致处理器核心数量的增加而非频率的提升。

---

## 9. 未来的研究挑战
- **超级计算机的动态与适应性**：使超级计算机能够更加动态和自适应，从而提高未来的扩展能力。
- **更易编程的超级计算机**：引入抽象的、与机器无关的编程模型，使编程更加容易。
- **数据密集型计算的改进**：提升数据中心在计算密集型任务中的性能，更好地利用数据局部性。
- **技术与经济的挑战**：在 Moore 定律终结后，如何维持 CMOS 制造工艺的稳定性。

---

## 10. 小结
- **高性能计算的未来**涉及计算密集型和数据密集型应用的逐渐融合。
- 当前，超级计算机和数据中心集群在硬件、运行时系统和应用编程方面采取了不同的路径演化。
- **融合的潜力**：未来如果能成功融合两者的优点，将带来重要的计算能力和经济效益，但如何实现这一点仍有诸多挑战。















































# 第 27 课：数据库系统的扩展挑战（Database Systems Do Not Scale to 1000 CPU Cores）

---

## 1. 课程概述
- 本节课讨论了为什么**数据库系统（DBMS）**难以扩展到上千个 CPU 核心，特别是在处理高并发事务时遇到的瓶颈。
- 重点介绍了数据库事务处理、并发控制机制以及这些机制在多核系统中的扩展性问题。

---

## 2. 为什么扩展性很重要
- **单核 CPU 提速的时代已经结束**，摩尔定律放缓，计算机性能的提升更多依赖于增加核心数量。
- 数据库应用变得越来越复杂和庞大，需要**许多核心**来支持更大的并行度。
- 然而，现有的数据库管理系统（DBMS）在面对未来的“**多核 CPU 架构**”时，未能有效利用这些额外的计算资源。

---

## 3. 数据库事务处理
### 3.1 在线事务处理（OLTP）
- OLTP（**On-line Transaction Processing**）是数据库的主要应用场景之一，处理快速的操作，涉及数据的插入、更新或删除。
- 事务操作必须满足 ACID 属性：
  - **原子性（Atomicity）**：事务要么全部完成，要么全部不执行。
  - **一致性（Consistency）**：事务执行前后，数据库保持一致。
  - **隔离性（Isolation）**：每个事务好像在独立执行，互不干扰。
  - **持久性（Durability）**：事务一旦提交，其效果是永久的。

---

## 4. 并发控制机制
### 4.1 两阶段锁定协议（Two-Phase Locking, 2PL）
- **两阶段锁定（2PL）**：
  - **扩展阶段**：事务在这个阶段获取锁。
  - **收缩阶段**：事务在这个阶段释放锁。
  - 两阶段的特性使得事务能够保证原子性和隔离性。

- **死锁处理**：
  - **死锁检测（Deadlock Detection, DL_DETECT）**：检测系统中的死锁并进行恢复。
  - **非等待死锁预防（NO_WAIT）**：若请求的锁已被占用，则立即中止当前事务。
  - **等待并死亡策略（WAIT_DIE）**：若请求的锁已被占用，当前事务可选择等待或终止自己。

### 4.2 时间戳排序（Timestamp Ordering, T/O）
- 每个事务在开始时分配一个唯一的时间戳，用于决定事务的执行顺序。
- **基本时间戳排序**和**多版本并发控制（MVCC）**：
  - **MVCC** 允许对数据进行多版本存储，以便于事务读取之前的版本，从而提高并发性。
  - **乐观并发控制（Optimistic Concurrency Control, OCC）**：假设事务冲突概率低，只有在提交阶段检查冲突。

---

## 5. 扩展中的瓶颈
### 5.1 锁抖动（Lock Thrashing）
- **锁抖动**发生在高并发情况下，当多个事务频繁争用相同的锁时会造成严重的等待链。
- **理想情况**是所有事务按照主键顺序获取锁，但实际系统中往往难以实现这一点。

### 5.2 时间戳分配和内存分配
- **时间戳分配**：所有 T/O 算法和 `WAIT_DIE` 策略都依赖于全局时间戳分配，这会造成性能瓶颈。
- **内存分配**：对于 OCC 和 MVCC，在高并发的情况下，内存分配的开销也会显著增加，影响整体系统性能。

---

## 6. 实验平台
### 6.1 DBx1000 和 Graphite Simulator
- 使用 **DBx1000** 在 **Graphite Simulator** 上进行实验，模拟不同的并发控制方案在多核环境下的表现。
- **实验场景**：采用 **Yahoo! Cloud Serving Benchmark (YCSB)**，包含 2000 万个数据项，每个数据项为 1 KB，总数据库大小约 20 GB。
- 每个事务读取或修改 16 个数据项，模拟了不同程度的访问偏差和事务竞争。

### 6.2 实验结果
- 在 512 核的情况下，写密集型工作负载中的瓶颈主要包括：
  - **锁抖动（Lock Thrashing）**：例如在使用 DL_DETECT 和 WAIT_DIE 时显得尤为严重。
  - **时间戳分配**：T/O 算法依赖全局时间戳，成为瓶颈。
  - **内存分配**：对 MVCC 和 OCC 来说，高频的内存分配导致性能下降。

---

## 7. 硬件/软件协同设计
- **新的硬件优化**可以解决现有数据库系统在多核环境中的瓶颈：
  - **硬件加速的锁共享（Hardware-accelerated Lock Sharing）**：利用硬件来优化锁的获取和释放过程，减少锁抖动。
  - **异步内存复制（Asynchronous Memory Copying）**：减少事务处理中的内存操作延迟。
  - **去中心化内存控制器（Decentralized Memory Controller）**：避免全局控制器成为性能瓶颈。

---

## 8. 下一步研究
- **日志和恢复**：对数据库系统的日志记录和故障恢复进行优化，以提高容错性和持久性。
- **索引优化**：改进并行索引结构，以提高数据库在多核环境下的查询和更新性能。
- **扩展 DBx1000** 以支持分布式并发控制算法，提高大规模分布式数据库的性能和可靠性。

---

## 9. 小结
- **数据库系统的扩展挑战**在于如何利用现代多核处理器的计算能力来有效地处理并发事务。
- 现有的并发控制机制在多核环境下面临严重的性能瓶颈，包括锁抖动、时间戳分配和内存分配。
- 未来需要结合**硬件与软件的协同优化**，从硬件层面加速并发控制操作，从而克服数据库在多核环境中的性能瓶颈。



























































































# 第 28 课：课程总结与项目展示技巧（Course Wrap Up & Project Presentation Tips）

---

## 1. 课程总结
- 本节课为 **CMU 15-418/15-618 并行计算机体系结构与编程**课程的总结，重点回顾了本学期的核心内容和重要概念，并对课程项目的展示进行了指导。
- 主要内容包括：
  - 第 2 次考试的讨论
  - 并行性竞赛提示
  - 课程的整体总结与项目展示技巧

---

## 2. 课程的主要议题
### 2.1 并行计算的关键问题
- 课程涵盖了以下关键问题：
  - **并行性识别**：或相应地，识别计算中的依赖性。
  - **并行性调度**：如何有效地分配并行任务。
    1. **工作负载均衡**：确保所有处理单元都能尽可能地保持繁忙。
    2. **克服通信约束**：包括带宽限制、延迟处理、以及同步问题。
  - **利用数据/计算的局部性**：即如何有效地管理状态和数据。

### 2.2 涵盖的计算架构
- 课程在不同的架构上探讨了并行计算的问题和方法：
  - **异构移动 SoC**（如多核 CPU + 多核 GPU + 特殊的媒体加速器）。
  - **单芯片多核 CPU**。
  - **多核 GPU**。
  - **CPU + GPU 通过总线连接**。
  - **机器集群**。
  - **大规模、多节点超级计算机**。

### 2.3 并行计算硬件的未来
- 未来获得更高性能计算硬件的主要方式是通过**增加并行性**和**硬件特化（Hardware Specialization）**。
- 当前的硬件实例包括：
  - **Intel Xeon Phi**：72 核，16 宽 SIMD，4 路多线程。
  - **NVIDIA Maxwell GPU**：单个 SMM 核心，32 宽 SIMD，2048 CUDA 核心线程。
  - **Apple A9 芯片**：多核 CPU + 多核 GPU + 媒体 ASICs。
  - **FPGA**：可重构逻辑，支持针对特定工作负载的硬件加速。

---

## 3. 项目展示技巧
### 3.1 提高展示影响力的理由
- **清晰的展示**可以显著增加你工作对观众的影响力：
  - 其他人更有可能记住并基于你的工作进行改进。
  - 观众更有可能在展示后与你进一步交流。

### 3.2 最重要的原则：清晰而非全面
- 展示时应该**优先考虑清晰**，而非试图涵盖项目的所有细节。
- 详细内容可以在项目报告中展现，展示则应专注于传达最重要的信息。

### 3.3 具体展示建议
1. **从观众的角度出发**：
   - 你需要设想，观众是否能理解你说的内容，尤其是那些没有与你一起做项目的人。
   
2. **确定重点**：
   - 在展示中挑选一个焦点，然后只讨论这个重点。每个句子都应该有明确的目标。

3. **尽量降低观众的认知负担**：
   - 不要让观众去自己解读图表或图形，应直接告诉他们如何理解每个元素的意义。
   - 在解释图表时，应依次介绍图中的元素（如坐标轴、曲线等），并直接指明图表要传达的核心信息。

4. **设置问题背景**：
   - 明确展示问题的输入、输出和限制条件（目标与假设）。
   - “给定这些输入，我们希望得到这些输出……”
   - 例如，输出应具有某些特性，算法需要实时运行，或者系统需要与某个现有库进行互操作。

5. **描述系统时的顺序**：
   - 先介绍名词（系统中的主要组件和实体）。
   - 然后描述动词（对系统状态的操作，如何更新、处理或转换这些状态）。

6. **给出“为什么”再讲“是什么”**：
   - 在介绍某个实现细节之前，先解释为什么需要引入它。
   - 例如，在描述优化时，先说明为什么需要这种优化，然后再详细介绍优化的具体内容。

7. **图表的讲解**：
   - 每一个图表都应该被详细解释，尤其是视觉元素为何存在以及它们传递的信息。
   - 例如，“这里展示了两个相邻的三角形，我们给每个像素着色，表明每个像素在渲染过程中发生了几次着色。”

8. **每张结果图表只传递一个信息**：
   - 每张幻灯片应只包含一个关键点，且这个关键点应该被放在幻灯片的标题中。

9. **幻灯片标题的重要性**：
   - 一个好的幻灯片标题能总结该页内容，使得观众不需要费力去理解。

10. **展示中应展示出你所做工作的成果**：
    - 例如，通过和发表的结果对比，说明你的代码比某个知名系统快了 10%。
    - 对结果的真实对比非常重要，应避免夸大结论，例如简单地比较 GPU 和单线程 CPU 的运行速度。

11. **反复练习你的展示**：
    - 在有限的时间内进行流畅展示需要大量的练习，建议在展示前多次演练，确保每个环节都顺利进行。

---

## 4. 最后的话
- 未来计算硬件的发展方向仍然是**增加并行性和硬件特化**，但目前的软件普遍未能充分利用现代硬件的能力。
- 提高性能的挑战不仅在于硬件的设计，还在于如何让程序员能够**更轻松地利用这些复杂的硬件**。
- 课程中教授的内容和概念，为未来应对复杂计算系统的挑战提供了理论基础和实际经验。

